{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T07:40:16.952559Z",
     "start_time": "2018-06-14T07:40:15.478327Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import torch.utils.data as utils\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import time\n",
    "import os\n",
    "from torch.utils import data\n",
    "from wavenet import Wavenet\n",
    "from transformData import x_mu_law_encode,y_mu_law_encode,mu_law_decode,onehot,cateToSignal\n",
    "from readDataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T07:40:16.973266Z",
     "start_time": "2018-06-14T07:40:16.955651Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sampleSize=32000#the length of the sample size\n",
    "quantization_channels=256\n",
    "sample_rate=16000\n",
    "dilations=[2**i for i in range(9)]*5  #idea from wavenet, have more receptive field\n",
    "residualDim=128 #\n",
    "skipDim=512\n",
    "shapeoftest = 190500\n",
    "filterSize=3\n",
    "resumefile='testac' # name of checkpoint\n",
    "lossname='testacloss.txt' # name of loss file\n",
    "continueTrain=False # whether use checkpoint\n",
    "pad = np.sum(dilations) # padding for dilate convolutional layers\n",
    "lossrecord=[]  #list for record loss\n",
    "pad=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    #            |----------------------------------------|     *residual*\n",
    "    #            |                                        |\n",
    "    #            |    |-- conv -- tanh --|                |\n",
    "    # -> dilate -|----|                  * ----|-- 1x1 -- + -->\t*input*\n",
    "    #                 |-- conv -- sigm --|     |    ||\n",
    "    #                                         1x1=residualDim\n",
    "    #                                          |\n",
    "    # ---------------------------------------> + ------------->\t*skip=skipDim*\n",
    "    image changed from https://github.com/vincentherrmann/pytorch-wavenet/blob/master/wavenet_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T07:40:16.980188Z",
     "start_time": "2018-06-14T07:40:16.977115Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"  # use specific GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T07:40:17.001974Z",
     "start_time": "2018-06-14T07:40:16.982867Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available() # whether have available GPU\n",
    "torch.manual_seed(1)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "#device = 'cpu'\n",
    "#torch.set_default_tensor_type('torch.cuda.FloatTensor') #set_default_tensor_type as cuda tensor\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T07:40:17.019133Z",
     "start_time": "2018-06-14T07:40:17.011107Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {'batch_size': 1,'shuffle': True,'num_workers': 1}\n",
    "training_set = Dataset(['origin_mix'],['origin_vocal'],'./vsCorpus/','./vsCorpus/')\n",
    "testing_set = Dataset(['pred_mix'],['pred_mix'],'./vsCorpus/','./vsCorpus/')\n",
    "loadtr = data.DataLoader(training_set, **params) #pytorch dataloader, more faster than mine\n",
    "loadval = data.DataLoader(testing_set, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T07:40:22.214459Z",
     "start_time": "2018-06-14T07:40:17.021995Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Wavenet(pad,skipDim,quantization_channels,residualDim,dilations).cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#in wavenet paper, they said crossentropyloss is far better than MSELoss\n",
    "#optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3,weight_decay=1e-5)\n",
    "#use adam to train\n",
    "#optimizer = optim.SGD(model.parameters(), lr = 0.1, momentum=0.9, weight_decay=1e-5)\n",
    "#scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "#scheduler = MultiStepLR(optimizer, milestones=[20,40], gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T07:40:22.226549Z",
     "start_time": "2018-06-14T07:40:22.216825Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if continueTrain:# if continueTrain, the program will find the checkpoints\n",
    "    if os.path.isfile(resumefile):\n",
    "        print(\"=> loading checkpoint '{}'\".format(resumefile))\n",
    "        checkpoint = torch.load(resumefile)\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        #best_prec1 = checkpoint['best_prec1']\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "              .format(resumefile, checkpoint['epoch']))\n",
    "    else:\n",
    "        print(\"=> no checkpoint found at '{}'\".format(resumefile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-06-14T07:40:15.483Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def val(xtrain,ytrain): #validation last 15 seconds of the audio.\n",
    "    model.eval()\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        idx = np.arange(xtrain.shape[-1]-pad-10*sampleSize,xtrain.shape[-1]-pad-sampleSize,1000)\n",
    "        np.random.shuffle(idx)\n",
    "        data = xtrain[:,:,idx[0]-pad:pad+idx[0]+sampleSize].to(device)\n",
    "        target = ytrain[:,idx[0]:idx[0]+sampleSize].to(device)\n",
    "        output = model(data)\n",
    "        pred = output.max(1, keepdim=True)[1]\n",
    "        correct = pred.eq(target.view_as(pred)).sum().item() / pred.shape[-1]\n",
    "        val_loss = criterion(output, target).item()\n",
    "        print(correct,'accurate')\n",
    "        print('\\nval set:loss{:.4f}:, ({:.3f} sec/step)\\n'.format(val_loss,time.time()-start_time))\n",
    "        \n",
    "        listofpred = []\n",
    "        for ind in range(xtrain.shape[-1]-pad-10*sampleSize,xtrain.shape[-1]-pad-sampleSize,sampleSize):\n",
    "            output = model(xtrain[:, :, ind - pad:ind + sampleSize + pad].to(device))\n",
    "            pred = output.max(1, keepdim=True)[1].cpu().numpy().reshape(-1)\n",
    "            listofpred.append(pred)\n",
    "        ans = mu_law_decode(np.concatenate(listofpred))\n",
    "        sf.write('./vsCorpus/notexval.wav', ans, sample_rate)\n",
    "        print('val stored done',time.time() - start_time)\n",
    "        \n",
    "\n",
    "def test(xtrain):# testing data\n",
    "    model.eval()\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for iloader,(xtest,_) in enumerate(loadval):\n",
    "            listofpred = []\n",
    "            for ind in range(pad, xtest.shape[-1] - pad, sampleSize):\n",
    "                output = model(xtest[:, :, ind - pad:ind + sampleSize + pad].to(device))\n",
    "                pred = output.max(1, keepdim=True)[1].cpu().numpy().reshape(-1)\n",
    "                listofpred.append(pred)\n",
    "            ans = mu_law_decode(np.concatenate(listofpred))\n",
    "            sf.write('./vsCorpus/notexte.wav', ans, sample_rate)\n",
    "\n",
    "            listofpred=[]\n",
    "            for ind in range(pad,xtrain.shape[-1]-pad,sampleSize):\n",
    "                output = model(xtrain[:, :, ind-pad:ind+sampleSize+pad].to(device))\n",
    "                pred = output.max(1, keepdim=True)[1].cpu().numpy().reshape(-1)\n",
    "                listofpred.append(pred)\n",
    "            ans = mu_law_decode(np.concatenate(listofpred))\n",
    "            sf.write('./vsCorpus/notextr.wav', ans, sample_rate)\n",
    "            print('test stored done',time.time() - start_time)\n",
    "    \n",
    "def train(epoch):#training data, the audio except for last 15 seconds\n",
    "    model.train()\n",
    "    for iloader,(xtrain,ytrain) in enumerate(loadtr):\n",
    "        idx = np.arange(pad,xtrain.shape[-1]-pad-11*sampleSize,16000)\n",
    "        np.random.shuffle(idx)#random the starting points\n",
    "        for i, ind in enumerate(idx):\n",
    "            start_time = time.time()\n",
    "            data, target = xtrain[:,:,ind-pad:ind+sampleSize+pad].to(device), ytrain[:,ind:ind+sampleSize].to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            lossrecord.append(loss.item())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)] Loss:{:.6f}: , ({:.3f} sec/step)'.format(\n",
    "                    epoch, i, len(idx),100. * i / len(idx), loss.item(),time.time() - start_time))\n",
    "            if i % 100 == 0:\n",
    "                with open(\"./lossRecord/\"+lossname, \"w\") as f:\n",
    "                    for s in lossrecord:\n",
    "                        f.write(str(s) +\"\\n\")\n",
    "                print('write finish')\n",
    "\n",
    "        val(xtrain,ytrain)\n",
    "        test(xtrain)\n",
    "        state={'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict()}\n",
    "        torch.save(state, './model/'+resumefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-06-14T07:40:15.488Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/214 (0%)] Loss:5.566586: , (1.864 sec/step)\n",
      "write finish\n",
      "Train Epoch: 0 [1/214 (0%)] Loss:5.516381: , (1.831 sec/step)\n",
      "Train Epoch: 0 [2/214 (1%)] Loss:5.634708: , (1.838 sec/step)\n",
      "Train Epoch: 0 [3/214 (1%)] Loss:5.480080: , (1.842 sec/step)\n",
      "Train Epoch: 0 [4/214 (2%)] Loss:5.410698: , (1.834 sec/step)\n",
      "Train Epoch: 0 [5/214 (2%)] Loss:5.411734: , (1.850 sec/step)\n",
      "Train Epoch: 0 [6/214 (3%)] Loss:5.263039: , (1.838 sec/step)\n",
      "Train Epoch: 0 [7/214 (3%)] Loss:5.223952: , (1.838 sec/step)\n",
      "Train Epoch: 0 [8/214 (4%)] Loss:5.167505: , (1.841 sec/step)\n",
      "Train Epoch: 0 [9/214 (4%)] Loss:5.183545: , (1.859 sec/step)\n",
      "Train Epoch: 0 [10/214 (5%)] Loss:5.231439: , (1.849 sec/step)\n",
      "Train Epoch: 0 [11/214 (5%)] Loss:5.240126: , (1.856 sec/step)\n",
      "Train Epoch: 0 [12/214 (6%)] Loss:5.267186: , (1.855 sec/step)\n",
      "Train Epoch: 0 [13/214 (6%)] Loss:5.194458: , (1.865 sec/step)\n",
      "Train Epoch: 0 [14/214 (7%)] Loss:5.176863: , (1.868 sec/step)\n",
      "Train Epoch: 0 [15/214 (7%)] Loss:5.090603: , (1.862 sec/step)\n",
      "Train Epoch: 0 [16/214 (7%)] Loss:4.970299: , (1.871 sec/step)\n",
      "Train Epoch: 0 [17/214 (8%)] Loss:5.329985: , (1.861 sec/step)\n",
      "Train Epoch: 0 [18/214 (8%)] Loss:5.232744: , (1.863 sec/step)\n",
      "Train Epoch: 0 [19/214 (9%)] Loss:5.227627: , (1.860 sec/step)\n",
      "Train Epoch: 0 [20/214 (9%)] Loss:5.098414: , (1.869 sec/step)\n",
      "Train Epoch: 0 [21/214 (10%)] Loss:5.106627: , (1.865 sec/step)\n",
      "Train Epoch: 0 [22/214 (10%)] Loss:5.200748: , (1.854 sec/step)\n",
      "Train Epoch: 0 [23/214 (11%)] Loss:5.010579: , (1.851 sec/step)\n",
      "Train Epoch: 0 [24/214 (11%)] Loss:5.044039: , (1.859 sec/step)\n",
      "Train Epoch: 0 [25/214 (12%)] Loss:4.968312: , (1.857 sec/step)\n",
      "Train Epoch: 0 [26/214 (12%)] Loss:5.359277: , (1.847 sec/step)\n",
      "Train Epoch: 0 [27/214 (13%)] Loss:5.124589: , (1.848 sec/step)\n",
      "Train Epoch: 0 [28/214 (13%)] Loss:5.122038: , (1.846 sec/step)\n",
      "Train Epoch: 0 [29/214 (14%)] Loss:5.151999: , (1.859 sec/step)\n",
      "Train Epoch: 0 [30/214 (14%)] Loss:5.202778: , (1.845 sec/step)\n",
      "Train Epoch: 0 [31/214 (14%)] Loss:4.982408: , (1.844 sec/step)\n",
      "Train Epoch: 0 [32/214 (15%)] Loss:5.092371: , (1.842 sec/step)\n",
      "Train Epoch: 0 [33/214 (15%)] Loss:4.960336: , (1.838 sec/step)\n",
      "Train Epoch: 0 [34/214 (16%)] Loss:4.994936: , (1.837 sec/step)\n",
      "Train Epoch: 0 [35/214 (16%)] Loss:5.047496: , (1.837 sec/step)\n",
      "Train Epoch: 0 [36/214 (17%)] Loss:4.971624: , (1.837 sec/step)\n",
      "Train Epoch: 0 [37/214 (17%)] Loss:5.023152: , (1.837 sec/step)\n",
      "Train Epoch: 0 [38/214 (18%)] Loss:4.967446: , (1.833 sec/step)\n",
      "Train Epoch: 0 [39/214 (18%)] Loss:4.976665: , (1.833 sec/step)\n",
      "Train Epoch: 0 [40/214 (19%)] Loss:4.968915: , (1.835 sec/step)\n",
      "Train Epoch: 0 [41/214 (19%)] Loss:4.975600: , (1.837 sec/step)\n",
      "Train Epoch: 0 [42/214 (20%)] Loss:5.059609: , (1.838 sec/step)\n",
      "Train Epoch: 0 [43/214 (20%)] Loss:5.142070: , (1.840 sec/step)\n",
      "Train Epoch: 0 [44/214 (21%)] Loss:5.018089: , (1.840 sec/step)\n",
      "Train Epoch: 0 [45/214 (21%)] Loss:4.897173: , (1.846 sec/step)\n",
      "Train Epoch: 0 [46/214 (21%)] Loss:4.840197: , (1.855 sec/step)\n",
      "Train Epoch: 0 [47/214 (22%)] Loss:4.908782: , (1.845 sec/step)\n",
      "Train Epoch: 0 [48/214 (22%)] Loss:5.064493: , (1.849 sec/step)\n",
      "Train Epoch: 0 [49/214 (23%)] Loss:4.875298: , (1.847 sec/step)\n",
      "Train Epoch: 0 [50/214 (23%)] Loss:4.965247: , (1.849 sec/step)\n",
      "Train Epoch: 0 [51/214 (24%)] Loss:4.863083: , (1.851 sec/step)\n",
      "Train Epoch: 0 [52/214 (24%)] Loss:5.053570: , (1.851 sec/step)\n",
      "Train Epoch: 0 [53/214 (25%)] Loss:5.158782: , (1.853 sec/step)\n",
      "Train Epoch: 0 [54/214 (25%)] Loss:4.934203: , (1.860 sec/step)\n",
      "Train Epoch: 0 [55/214 (26%)] Loss:5.058621: , (1.853 sec/step)\n",
      "Train Epoch: 0 [56/214 (26%)] Loss:4.990731: , (1.855 sec/step)\n",
      "Train Epoch: 0 [57/214 (27%)] Loss:4.935581: , (1.855 sec/step)\n",
      "Train Epoch: 0 [58/214 (27%)] Loss:4.931164: , (1.855 sec/step)\n",
      "Train Epoch: 0 [59/214 (28%)] Loss:5.306333: , (1.855 sec/step)\n",
      "Train Epoch: 0 [60/214 (28%)] Loss:5.355632: , (1.853 sec/step)\n",
      "Train Epoch: 0 [61/214 (29%)] Loss:4.997039: , (1.853 sec/step)\n",
      "Train Epoch: 0 [62/214 (29%)] Loss:4.870289: , (1.853 sec/step)\n",
      "Train Epoch: 0 [63/214 (29%)] Loss:4.832560: , (1.852 sec/step)\n",
      "Train Epoch: 0 [64/214 (30%)] Loss:4.963351: , (1.861 sec/step)\n",
      "Train Epoch: 0 [65/214 (30%)] Loss:4.812402: , (1.860 sec/step)\n",
      "Train Epoch: 0 [66/214 (31%)] Loss:4.882544: , (1.851 sec/step)\n",
      "Train Epoch: 0 [67/214 (31%)] Loss:4.780364: , (1.850 sec/step)\n",
      "Train Epoch: 0 [68/214 (32%)] Loss:5.162490: , (1.850 sec/step)\n",
      "Train Epoch: 0 [69/214 (32%)] Loss:4.730893: , (1.854 sec/step)\n",
      "Train Epoch: 0 [70/214 (33%)] Loss:5.059223: , (1.856 sec/step)\n",
      "Train Epoch: 0 [71/214 (33%)] Loss:5.001639: , (1.857 sec/step)\n",
      "Train Epoch: 0 [72/214 (34%)] Loss:4.978379: , (1.848 sec/step)\n",
      "Train Epoch: 0 [73/214 (34%)] Loss:5.196826: , (1.846 sec/step)\n",
      "Train Epoch: 0 [74/214 (35%)] Loss:5.038303: , (1.846 sec/step)\n",
      "Train Epoch: 0 [75/214 (35%)] Loss:4.952627: , (1.857 sec/step)\n",
      "Train Epoch: 0 [76/214 (36%)] Loss:5.008500: , (1.845 sec/step)\n",
      "Train Epoch: 0 [77/214 (36%)] Loss:5.123522: , (1.844 sec/step)\n",
      "Train Epoch: 0 [78/214 (36%)] Loss:5.164632: , (1.843 sec/step)\n",
      "Train Epoch: 0 [79/214 (37%)] Loss:5.130189: , (1.851 sec/step)\n",
      "Train Epoch: 0 [80/214 (37%)] Loss:4.896732: , (1.841 sec/step)\n",
      "Train Epoch: 0 [81/214 (38%)] Loss:5.418050: , (1.841 sec/step)\n",
      "Train Epoch: 0 [82/214 (38%)] Loss:5.164224: , (1.841 sec/step)\n",
      "Train Epoch: 0 [83/214 (39%)] Loss:4.983422: , (1.852 sec/step)\n",
      "Train Epoch: 0 [84/214 (39%)] Loss:4.927927: , (1.846 sec/step)\n",
      "Train Epoch: 0 [85/214 (40%)] Loss:5.073545: , (1.846 sec/step)\n",
      "Train Epoch: 0 [86/214 (40%)] Loss:5.102233: , (1.845 sec/step)\n",
      "Train Epoch: 0 [87/214 (41%)] Loss:5.105272: , (1.847 sec/step)\n",
      "Train Epoch: 0 [88/214 (41%)] Loss:4.945664: , (1.853 sec/step)\n",
      "Train Epoch: 0 [89/214 (42%)] Loss:5.059714: , (1.860 sec/step)\n",
      "Train Epoch: 0 [90/214 (42%)] Loss:4.911227: , (1.851 sec/step)\n",
      "Train Epoch: 0 [91/214 (43%)] Loss:4.963941: , (1.852 sec/step)\n",
      "Train Epoch: 0 [92/214 (43%)] Loss:5.004082: , (1.862 sec/step)\n",
      "Train Epoch: 0 [93/214 (43%)] Loss:4.818831: , (1.854 sec/step)\n",
      "Train Epoch: 0 [94/214 (44%)] Loss:4.811896: , (1.859 sec/step)\n",
      "Train Epoch: 0 [95/214 (44%)] Loss:4.835231: , (1.853 sec/step)\n",
      "Train Epoch: 0 [96/214 (45%)] Loss:4.957495: , (1.861 sec/step)\n",
      "Train Epoch: 0 [97/214 (45%)] Loss:4.864931: , (1.865 sec/step)\n",
      "Train Epoch: 0 [98/214 (46%)] Loss:4.895047: , (1.919 sec/step)\n",
      "Train Epoch: 0 [99/214 (46%)] Loss:4.879386: , (1.929 sec/step)\n",
      "Train Epoch: 0 [100/214 (47%)] Loss:4.910728: , (1.916 sec/step)\n",
      "write finish\n",
      "Train Epoch: 0 [101/214 (47%)] Loss:5.065266: , (1.911 sec/step)\n",
      "Train Epoch: 0 [102/214 (48%)] Loss:4.785357: , (1.834 sec/step)\n",
      "Train Epoch: 0 [103/214 (48%)] Loss:4.729354: , (1.838 sec/step)\n",
      "Train Epoch: 0 [104/214 (49%)] Loss:5.035008: , (1.828 sec/step)\n",
      "Train Epoch: 0 [105/214 (49%)] Loss:4.886727: , (1.828 sec/step)\n",
      "Train Epoch: 0 [106/214 (50%)] Loss:4.797159: , (1.840 sec/step)\n",
      "Train Epoch: 0 [107/214 (50%)] Loss:5.079321: , (1.833 sec/step)\n",
      "Train Epoch: 0 [108/214 (50%)] Loss:4.768780: , (1.845 sec/step)\n",
      "Train Epoch: 0 [109/214 (51%)] Loss:4.920373: , (1.839 sec/step)\n",
      "Train Epoch: 0 [110/214 (51%)] Loss:4.884660: , (1.838 sec/step)\n",
      "Train Epoch: 0 [111/214 (52%)] Loss:4.776250: , (1.841 sec/step)\n",
      "Train Epoch: 0 [112/214 (52%)] Loss:4.558613: , (1.843 sec/step)\n",
      "Train Epoch: 0 [113/214 (53%)] Loss:4.907923: , (1.852 sec/step)\n",
      "Train Epoch: 0 [114/214 (53%)] Loss:4.628220: , (1.849 sec/step)\n",
      "Train Epoch: 0 [115/214 (54%)] Loss:4.734193: , (1.858 sec/step)\n",
      "Train Epoch: 0 [116/214 (54%)] Loss:4.734918: , (1.860 sec/step)\n",
      "Train Epoch: 0 [117/214 (55%)] Loss:5.043026: , (1.851 sec/step)\n",
      "Train Epoch: 0 [118/214 (55%)] Loss:4.779641: , (1.854 sec/step)\n",
      "Train Epoch: 0 [119/214 (56%)] Loss:4.695261: , (1.855 sec/step)\n",
      "Train Epoch: 0 [120/214 (56%)] Loss:4.767404: , (1.853 sec/step)\n",
      "Train Epoch: 0 [121/214 (57%)] Loss:4.891401: , (1.855 sec/step)\n",
      "Train Epoch: 0 [122/214 (57%)] Loss:5.097373: , (1.860 sec/step)\n",
      "Train Epoch: 0 [123/214 (57%)] Loss:4.882323: , (1.855 sec/step)\n",
      "Train Epoch: 0 [124/214 (58%)] Loss:4.854405: , (1.854 sec/step)\n",
      "Train Epoch: 0 [125/214 (58%)] Loss:5.014115: , (1.864 sec/step)\n",
      "Train Epoch: 0 [126/214 (59%)] Loss:5.061154: , (1.863 sec/step)\n",
      "Train Epoch: 0 [127/214 (59%)] Loss:4.999075: , (1.863 sec/step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [128/214 (60%)] Loss:4.893009: , (1.851 sec/step)\n",
      "Train Epoch: 0 [129/214 (60%)] Loss:4.942778: , (1.849 sec/step)\n",
      "Train Epoch: 0 [130/214 (61%)] Loss:4.994743: , (1.849 sec/step)\n",
      "Train Epoch: 0 [131/214 (61%)] Loss:4.764468: , (1.846 sec/step)\n",
      "Train Epoch: 0 [132/214 (62%)] Loss:4.766168: , (1.845 sec/step)\n",
      "Train Epoch: 0 [133/214 (62%)] Loss:4.715384: , (1.842 sec/step)\n",
      "Train Epoch: 0 [134/214 (63%)] Loss:4.839019: , (1.843 sec/step)\n",
      "Train Epoch: 0 [135/214 (63%)] Loss:4.801147: , (1.839 sec/step)\n",
      "Train Epoch: 0 [136/214 (64%)] Loss:4.671033: , (1.838 sec/step)\n",
      "Train Epoch: 0 [137/214 (64%)] Loss:5.035991: , (1.836 sec/step)\n",
      "Train Epoch: 0 [138/214 (64%)] Loss:4.726390: , (1.844 sec/step)\n",
      "Train Epoch: 0 [139/214 (65%)] Loss:4.889451: , (1.843 sec/step)\n",
      "Train Epoch: 0 [140/214 (65%)] Loss:5.005937: , (1.842 sec/step)\n",
      "Train Epoch: 0 [141/214 (66%)] Loss:4.880929: , (1.845 sec/step)\n",
      "Train Epoch: 0 [142/214 (66%)] Loss:4.770217: , (1.835 sec/step)\n",
      "Train Epoch: 0 [143/214 (67%)] Loss:4.845460: , (1.835 sec/step)\n",
      "Train Epoch: 0 [144/214 (67%)] Loss:4.870242: , (1.890 sec/step)\n",
      "Train Epoch: 0 [145/214 (68%)] Loss:5.189260: , (1.934 sec/step)\n",
      "Train Epoch: 0 [146/214 (68%)] Loss:4.692554: , (1.932 sec/step)\n",
      "Train Epoch: 0 [147/214 (69%)] Loss:4.719900: , (1.865 sec/step)\n",
      "Train Epoch: 0 [148/214 (69%)] Loss:4.656724: , (1.825 sec/step)\n",
      "Train Epoch: 0 [149/214 (70%)] Loss:4.925258: , (1.826 sec/step)\n",
      "Train Epoch: 0 [150/214 (70%)] Loss:4.907971: , (1.828 sec/step)\n",
      "Train Epoch: 0 [151/214 (71%)] Loss:4.885403: , (1.839 sec/step)\n",
      "Train Epoch: 0 [152/214 (71%)] Loss:4.892428: , (1.831 sec/step)\n",
      "Train Epoch: 0 [153/214 (71%)] Loss:4.791091: , (1.833 sec/step)\n",
      "Train Epoch: 0 [154/214 (72%)] Loss:4.704854: , (1.841 sec/step)\n",
      "Train Epoch: 0 [155/214 (72%)] Loss:4.718494: , (1.850 sec/step)\n",
      "Train Epoch: 0 [156/214 (73%)] Loss:4.748775: , (1.840 sec/step)\n",
      "Train Epoch: 0 [157/214 (73%)] Loss:4.635575: , (1.841 sec/step)\n",
      "Train Epoch: 0 [158/214 (74%)] Loss:4.679808: , (1.843 sec/step)\n",
      "Train Epoch: 0 [159/214 (74%)] Loss:4.824918: , (1.854 sec/step)\n",
      "Train Epoch: 0 [160/214 (75%)] Loss:4.816004: , (1.857 sec/step)\n",
      "Train Epoch: 0 [161/214 (75%)] Loss:4.611068: , (1.853 sec/step)\n",
      "Train Epoch: 0 [162/214 (76%)] Loss:4.723984: , (1.851 sec/step)\n",
      "Train Epoch: 0 [163/214 (76%)] Loss:4.884630: , (1.852 sec/step)\n",
      "Train Epoch: 0 [164/214 (77%)] Loss:5.254170: , (1.862 sec/step)\n",
      "Train Epoch: 0 [165/214 (77%)] Loss:4.859691: , (1.855 sec/step)\n",
      "Train Epoch: 0 [166/214 (78%)] Loss:4.855966: , (1.855 sec/step)\n",
      "Train Epoch: 0 [167/214 (78%)] Loss:4.864164: , (1.855 sec/step)\n",
      "Train Epoch: 0 [168/214 (79%)] Loss:4.659914: , (1.856 sec/step)\n",
      "Train Epoch: 0 [169/214 (79%)] Loss:4.841806: , (1.858 sec/step)\n",
      "Train Epoch: 0 [170/214 (79%)] Loss:4.719754: , (1.854 sec/step)\n",
      "Train Epoch: 0 [171/214 (80%)] Loss:4.756890: , (1.856 sec/step)\n",
      "Train Epoch: 0 [172/214 (80%)] Loss:4.911643: , (1.854 sec/step)\n",
      "Train Epoch: 0 [173/214 (81%)] Loss:4.716124: , (1.854 sec/step)\n",
      "Train Epoch: 0 [174/214 (81%)] Loss:4.733590: , (1.854 sec/step)\n",
      "Train Epoch: 0 [175/214 (82%)] Loss:4.853010: , (1.852 sec/step)\n",
      "Train Epoch: 0 [176/214 (82%)] Loss:4.799252: , (1.855 sec/step)\n",
      "Train Epoch: 0 [177/214 (83%)] Loss:4.877000: , (1.851 sec/step)\n",
      "Train Epoch: 0 [178/214 (83%)] Loss:4.803976: , (1.850 sec/step)\n",
      "Train Epoch: 0 [179/214 (84%)] Loss:4.657453: , (1.847 sec/step)\n",
      "Train Epoch: 0 [180/214 (84%)] Loss:4.977327: , (1.858 sec/step)\n",
      "Train Epoch: 0 [181/214 (85%)] Loss:4.611544: , (1.846 sec/step)\n",
      "Train Epoch: 0 [182/214 (85%)] Loss:4.736331: , (1.851 sec/step)\n",
      "Train Epoch: 0 [183/214 (86%)] Loss:4.737432: , (1.851 sec/step)\n",
      "Train Epoch: 0 [184/214 (86%)] Loss:4.655441: , (1.838 sec/step)\n",
      "Train Epoch: 0 [185/214 (86%)] Loss:4.771703: , (1.835 sec/step)\n",
      "Train Epoch: 0 [186/214 (87%)] Loss:4.770738: , (1.835 sec/step)\n",
      "Train Epoch: 0 [187/214 (87%)] Loss:4.852218: , (1.834 sec/step)\n",
      "Train Epoch: 0 [188/214 (88%)] Loss:4.692223: , (1.833 sec/step)\n",
      "Train Epoch: 0 [189/214 (88%)] Loss:4.697710: , (1.833 sec/step)\n",
      "Train Epoch: 0 [190/214 (89%)] Loss:4.622570: , (1.834 sec/step)\n",
      "Train Epoch: 0 [191/214 (89%)] Loss:4.637115: , (1.842 sec/step)\n",
      "Train Epoch: 0 [192/214 (90%)] Loss:4.903591: , (1.838 sec/step)\n",
      "Train Epoch: 0 [193/214 (90%)] Loss:4.719544: , (1.843 sec/step)\n",
      "Train Epoch: 0 [194/214 (91%)] Loss:4.515447: , (1.840 sec/step)\n",
      "Train Epoch: 0 [195/214 (91%)] Loss:4.828576: , (1.844 sec/step)\n",
      "Train Epoch: 0 [196/214 (92%)] Loss:4.833011: , (1.843 sec/step)\n",
      "Train Epoch: 0 [197/214 (92%)] Loss:4.681321: , (1.851 sec/step)\n",
      "Train Epoch: 0 [198/214 (93%)] Loss:4.812334: , (1.846 sec/step)\n",
      "Train Epoch: 0 [199/214 (93%)] Loss:4.870335: , (1.846 sec/step)\n",
      "Train Epoch: 0 [200/214 (93%)] Loss:4.548633: , (1.848 sec/step)\n",
      "write finish\n",
      "Train Epoch: 0 [201/214 (94%)] Loss:4.542160: , (1.847 sec/step)\n",
      "Train Epoch: 0 [202/214 (94%)] Loss:4.691726: , (1.851 sec/step)\n",
      "Train Epoch: 0 [203/214 (95%)] Loss:4.943003: , (1.855 sec/step)\n",
      "Train Epoch: 0 [204/214 (95%)] Loss:4.607860: , (1.852 sec/step)\n",
      "Train Epoch: 0 [205/214 (96%)] Loss:4.801010: , (1.851 sec/step)\n",
      "Train Epoch: 0 [206/214 (96%)] Loss:4.726934: , (1.854 sec/step)\n",
      "Train Epoch: 0 [207/214 (97%)] Loss:4.748957: , (1.854 sec/step)\n",
      "Train Epoch: 0 [208/214 (97%)] Loss:4.700456: , (1.864 sec/step)\n",
      "Train Epoch: 0 [209/214 (98%)] Loss:4.619658: , (1.857 sec/step)\n",
      "Train Epoch: 0 [210/214 (98%)] Loss:4.793691: , (1.858 sec/step)\n",
      "Train Epoch: 0 [211/214 (99%)] Loss:4.871605: , (1.855 sec/step)\n",
      "Train Epoch: 0 [212/214 (99%)] Loss:4.489659: , (1.858 sec/step)\n",
      "Train Epoch: 0 [213/214 (100%)] Loss:4.621362: , (1.857 sec/step)\n",
      "0.0201875 accurate\n",
      "\n",
      "val set:loss4.6670:, (0.590 sec/step)\n",
      "\n",
      "val stored done 5.926646709442139\n",
      "test stored done 74.01042699813843\n",
      "Train Epoch: 1 [0/214 (0%)] Loss:4.527912: , (1.867 sec/step)\n",
      "write finish\n",
      "Train Epoch: 1 [1/214 (0%)] Loss:4.615172: , (1.856 sec/step)\n",
      "Train Epoch: 1 [2/214 (1%)] Loss:4.588618: , (1.855 sec/step)\n",
      "Train Epoch: 1 [3/214 (1%)] Loss:4.821775: , (1.853 sec/step)\n",
      "Train Epoch: 1 [4/214 (2%)] Loss:4.542490: , (1.846 sec/step)\n",
      "Train Epoch: 1 [5/214 (2%)] Loss:4.747687: , (1.853 sec/step)\n",
      "Train Epoch: 1 [6/214 (3%)] Loss:4.627927: , (1.849 sec/step)\n",
      "Train Epoch: 1 [7/214 (3%)] Loss:4.612337: , (1.845 sec/step)\n",
      "Train Epoch: 1 [8/214 (4%)] Loss:4.574750: , (1.841 sec/step)\n",
      "Train Epoch: 1 [9/214 (4%)] Loss:4.703208: , (1.837 sec/step)\n",
      "Train Epoch: 1 [10/214 (5%)] Loss:4.627632: , (1.838 sec/step)\n",
      "Train Epoch: 1 [11/214 (5%)] Loss:4.494160: , (1.837 sec/step)\n",
      "Train Epoch: 1 [12/214 (6%)] Loss:4.682129: , (1.846 sec/step)\n",
      "Train Epoch: 1 [13/214 (6%)] Loss:4.641229: , (1.836 sec/step)\n",
      "Train Epoch: 1 [14/214 (7%)] Loss:4.543953: , (1.838 sec/step)\n",
      "Train Epoch: 1 [15/214 (7%)] Loss:4.656182: , (1.837 sec/step)\n",
      "Train Epoch: 1 [16/214 (7%)] Loss:4.587642: , (1.841 sec/step)\n",
      "Train Epoch: 1 [17/214 (8%)] Loss:4.477123: , (1.840 sec/step)\n",
      "Train Epoch: 1 [18/214 (8%)] Loss:4.529487: , (1.841 sec/step)\n",
      "Train Epoch: 1 [19/214 (9%)] Loss:4.496573: , (1.844 sec/step)\n",
      "Train Epoch: 1 [20/214 (9%)] Loss:4.471751: , (1.844 sec/step)\n",
      "Train Epoch: 1 [21/214 (10%)] Loss:4.549051: , (1.849 sec/step)\n",
      "Train Epoch: 1 [22/214 (10%)] Loss:4.605510: , (1.851 sec/step)\n",
      "Train Epoch: 1 [23/214 (11%)] Loss:4.406188: , (1.851 sec/step)\n",
      "Train Epoch: 1 [24/214 (11%)] Loss:5.063969: , (1.852 sec/step)\n",
      "Train Epoch: 1 [25/214 (12%)] Loss:4.559907: , (1.854 sec/step)\n",
      "Train Epoch: 1 [26/214 (12%)] Loss:4.676934: , (1.854 sec/step)\n",
      "Train Epoch: 1 [27/214 (13%)] Loss:4.674624: , (1.855 sec/step)\n",
      "Train Epoch: 1 [28/214 (13%)] Loss:4.620224: , (1.859 sec/step)\n",
      "Train Epoch: 1 [29/214 (14%)] Loss:4.581453: , (1.857 sec/step)\n",
      "Train Epoch: 1 [30/214 (14%)] Loss:4.544901: , (1.859 sec/step)\n",
      "Train Epoch: 1 [31/214 (14%)] Loss:4.617466: , (1.862 sec/step)\n",
      "Train Epoch: 1 [32/214 (15%)] Loss:4.697920: , (1.860 sec/step)\n",
      "Train Epoch: 1 [33/214 (15%)] Loss:4.811015: , (1.860 sec/step)\n",
      "Train Epoch: 1 [34/214 (16%)] Loss:4.591238: , (1.859 sec/step)\n",
      "Train Epoch: 1 [35/214 (16%)] Loss:4.792574: , (1.867 sec/step)\n",
      "Train Epoch: 1 [36/214 (17%)] Loss:4.642534: , (1.873 sec/step)\n",
      "Train Epoch: 1 [37/214 (17%)] Loss:4.643482: , (1.867 sec/step)\n",
      "Train Epoch: 1 [38/214 (18%)] Loss:4.650021: , (1.864 sec/step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [39/214 (18%)] Loss:4.451097: , (1.856 sec/step)\n",
      "Train Epoch: 1 [40/214 (19%)] Loss:4.728004: , (1.856 sec/step)\n",
      "Train Epoch: 1 [41/214 (19%)] Loss:4.665916: , (1.859 sec/step)\n",
      "Train Epoch: 1 [42/214 (20%)] Loss:4.629697: , (1.855 sec/step)\n",
      "Train Epoch: 1 [43/214 (20%)] Loss:4.466376: , (1.851 sec/step)\n",
      "Train Epoch: 1 [44/214 (21%)] Loss:4.820478: , (1.849 sec/step)\n",
      "Train Epoch: 1 [45/214 (21%)] Loss:4.631013: , (1.847 sec/step)\n",
      "Train Epoch: 1 [46/214 (21%)] Loss:4.585552: , (1.849 sec/step)\n",
      "Train Epoch: 1 [47/214 (22%)] Loss:4.574106: , (1.842 sec/step)\n",
      "Train Epoch: 1 [48/214 (22%)] Loss:4.618058: , (1.854 sec/step)\n",
      "Train Epoch: 1 [49/214 (23%)] Loss:4.645564: , (1.844 sec/step)\n",
      "Train Epoch: 1 [50/214 (23%)] Loss:4.445055: , (1.839 sec/step)\n",
      "Train Epoch: 1 [51/214 (24%)] Loss:4.422851: , (1.837 sec/step)\n",
      "Train Epoch: 1 [52/214 (24%)] Loss:4.290728: , (1.835 sec/step)\n",
      "Train Epoch: 1 [53/214 (25%)] Loss:4.741102: , (1.846 sec/step)\n",
      "Train Epoch: 1 [54/214 (25%)] Loss:4.477821: , (1.837 sec/step)\n",
      "Train Epoch: 1 [55/214 (26%)] Loss:4.730051: , (1.840 sec/step)\n",
      "Train Epoch: 1 [56/214 (26%)] Loss:4.549813: , (1.841 sec/step)\n",
      "Train Epoch: 1 [57/214 (27%)] Loss:4.491261: , (1.846 sec/step)\n",
      "Train Epoch: 1 [58/214 (27%)] Loss:4.684049: , (1.838 sec/step)\n",
      "Train Epoch: 1 [59/214 (28%)] Loss:4.531661: , (1.840 sec/step)\n",
      "Train Epoch: 1 [60/214 (28%)] Loss:4.400652: , (1.845 sec/step)\n",
      "Train Epoch: 1 [61/214 (29%)] Loss:4.486639: , (1.845 sec/step)\n",
      "Train Epoch: 1 [62/214 (29%)] Loss:4.710346: , (1.848 sec/step)\n",
      "Train Epoch: 1 [63/214 (29%)] Loss:4.525790: , (1.846 sec/step)\n",
      "Train Epoch: 1 [64/214 (30%)] Loss:4.600788: , (1.848 sec/step)\n",
      "Train Epoch: 1 [65/214 (30%)] Loss:4.401190: , (1.849 sec/step)\n",
      "Train Epoch: 1 [66/214 (31%)] Loss:5.099467: , (1.856 sec/step)\n",
      "Train Epoch: 1 [67/214 (31%)] Loss:4.598765: , (1.854 sec/step)\n",
      "Train Epoch: 1 [68/214 (32%)] Loss:4.466908: , (1.853 sec/step)\n",
      "Train Epoch: 1 [69/214 (32%)] Loss:4.613311: , (1.857 sec/step)\n",
      "Train Epoch: 1 [70/214 (33%)] Loss:4.475033: , (1.857 sec/step)\n",
      "Train Epoch: 1 [71/214 (33%)] Loss:4.614135: , (1.872 sec/step)\n",
      "Train Epoch: 1 [72/214 (34%)] Loss:4.454942: , (1.868 sec/step)\n",
      "Train Epoch: 1 [73/214 (34%)] Loss:4.723194: , (1.864 sec/step)\n",
      "Train Epoch: 1 [74/214 (35%)] Loss:4.438633: , (1.861 sec/step)\n",
      "Train Epoch: 1 [75/214 (35%)] Loss:4.507792: , (1.861 sec/step)\n",
      "Train Epoch: 1 [76/214 (36%)] Loss:4.548835: , (1.860 sec/step)\n",
      "Train Epoch: 1 [77/214 (36%)] Loss:4.660388: , (1.862 sec/step)\n",
      "Train Epoch: 1 [78/214 (36%)] Loss:4.541457: , (1.862 sec/step)\n",
      "Train Epoch: 1 [79/214 (37%)] Loss:4.612053: , (1.861 sec/step)\n",
      "Train Epoch: 1 [80/214 (37%)] Loss:4.501130: , (1.870 sec/step)\n",
      "Train Epoch: 1 [81/214 (38%)] Loss:4.587548: , (1.869 sec/step)\n",
      "Train Epoch: 1 [82/214 (38%)] Loss:4.490309: , (1.868 sec/step)\n",
      "Train Epoch: 1 [83/214 (39%)] Loss:4.455912: , (1.856 sec/step)\n",
      "Train Epoch: 1 [84/214 (39%)] Loss:4.562653: , (1.857 sec/step)\n",
      "Train Epoch: 1 [85/214 (40%)] Loss:4.614917: , (1.852 sec/step)\n",
      "Train Epoch: 1 [86/214 (40%)] Loss:4.630303: , (1.849 sec/step)\n",
      "Train Epoch: 1 [87/214 (41%)] Loss:4.406272: , (1.862 sec/step)\n",
      "Train Epoch: 1 [88/214 (41%)] Loss:4.724857: , (1.856 sec/step)\n",
      "Train Epoch: 1 [89/214 (42%)] Loss:4.873130: , (1.847 sec/step)\n",
      "Train Epoch: 1 [90/214 (42%)] Loss:4.550337: , (1.847 sec/step)\n",
      "Train Epoch: 1 [91/214 (43%)] Loss:4.712230: , (1.849 sec/step)\n",
      "Train Epoch: 1 [92/214 (43%)] Loss:4.685767: , (1.843 sec/step)\n",
      "Train Epoch: 1 [93/214 (43%)] Loss:4.669737: , (1.843 sec/step)\n",
      "Train Epoch: 1 [94/214 (44%)] Loss:4.656333: , (1.852 sec/step)\n",
      "Train Epoch: 1 [95/214 (44%)] Loss:4.735513: , (1.842 sec/step)\n",
      "Train Epoch: 1 [96/214 (45%)] Loss:4.587425: , (1.845 sec/step)\n",
      "Train Epoch: 1 [97/214 (45%)] Loss:4.411876: , (1.848 sec/step)\n",
      "Train Epoch: 1 [98/214 (46%)] Loss:4.950280: , (1.842 sec/step)\n",
      "Train Epoch: 1 [99/214 (46%)] Loss:4.494032: , (1.843 sec/step)\n",
      "Train Epoch: 1 [100/214 (47%)] Loss:4.769994: , (1.845 sec/step)\n",
      "write finish\n",
      "Train Epoch: 1 [101/214 (47%)] Loss:4.771900: , (1.849 sec/step)\n",
      "Train Epoch: 1 [102/214 (48%)] Loss:4.586530: , (1.847 sec/step)\n",
      "Train Epoch: 1 [103/214 (48%)] Loss:4.681075: , (1.848 sec/step)\n",
      "Train Epoch: 1 [104/214 (49%)] Loss:4.451488: , (1.848 sec/step)\n",
      "Train Epoch: 1 [105/214 (49%)] Loss:4.532776: , (1.847 sec/step)\n",
      "Train Epoch: 1 [106/214 (50%)] Loss:4.610803: , (1.855 sec/step)\n",
      "Train Epoch: 1 [107/214 (50%)] Loss:4.790569: , (1.848 sec/step)\n",
      "Train Epoch: 1 [108/214 (50%)] Loss:4.710981: , (1.848 sec/step)\n",
      "Train Epoch: 1 [109/214 (51%)] Loss:4.797724: , (1.852 sec/step)\n",
      "Train Epoch: 1 [110/214 (51%)] Loss:4.684984: , (1.850 sec/step)\n",
      "Train Epoch: 1 [111/214 (52%)] Loss:4.533847: , (1.863 sec/step)\n",
      "Train Epoch: 1 [112/214 (52%)] Loss:4.682881: , (1.852 sec/step)\n",
      "Train Epoch: 1 [113/214 (53%)] Loss:4.478433: , (1.853 sec/step)\n",
      "Train Epoch: 1 [114/214 (53%)] Loss:4.703799: , (1.859 sec/step)\n",
      "Train Epoch: 1 [115/214 (54%)] Loss:4.486691: , (1.852 sec/step)\n",
      "Train Epoch: 1 [116/214 (54%)] Loss:4.764972: , (1.851 sec/step)\n",
      "Train Epoch: 1 [117/214 (55%)] Loss:4.493992: , (1.853 sec/step)\n",
      "Train Epoch: 1 [118/214 (55%)] Loss:4.591977: , (1.850 sec/step)\n",
      "Train Epoch: 1 [119/214 (56%)] Loss:4.573228: , (1.849 sec/step)\n",
      "Train Epoch: 1 [120/214 (56%)] Loss:4.771607: , (1.848 sec/step)\n",
      "Train Epoch: 1 [121/214 (57%)] Loss:4.382930: , (1.857 sec/step)\n",
      "Train Epoch: 1 [122/214 (57%)] Loss:4.321816: , (1.857 sec/step)\n",
      "Train Epoch: 1 [123/214 (57%)] Loss:4.491147: , (1.843 sec/step)\n",
      "Train Epoch: 1 [124/214 (58%)] Loss:4.463172: , (1.844 sec/step)\n",
      "Train Epoch: 1 [125/214 (58%)] Loss:4.759590: , (1.850 sec/step)\n",
      "Train Epoch: 1 [126/214 (59%)] Loss:4.583071: , (1.841 sec/step)\n",
      "Train Epoch: 1 [127/214 (59%)] Loss:4.562309: , (1.840 sec/step)\n",
      "Train Epoch: 1 [128/214 (60%)] Loss:4.552966: , (1.839 sec/step)\n",
      "Train Epoch: 1 [129/214 (60%)] Loss:4.728528: , (1.850 sec/step)\n",
      "Train Epoch: 1 [130/214 (61%)] Loss:4.375814: , (1.840 sec/step)\n",
      "Train Epoch: 1 [131/214 (61%)] Loss:4.432080: , (1.850 sec/step)\n",
      "Train Epoch: 1 [132/214 (62%)] Loss:4.719823: , (1.840 sec/step)\n",
      "Train Epoch: 1 [133/214 (62%)] Loss:4.539057: , (1.849 sec/step)\n",
      "Train Epoch: 1 [134/214 (63%)] Loss:4.551715: , (1.838 sec/step)\n",
      "Train Epoch: 1 [135/214 (63%)] Loss:4.472709: , (1.839 sec/step)\n",
      "Train Epoch: 1 [136/214 (64%)] Loss:4.471559: , (1.838 sec/step)\n",
      "Train Epoch: 1 [137/214 (64%)] Loss:4.908482: , (1.844 sec/step)\n",
      "Train Epoch: 1 [138/214 (64%)] Loss:4.826848: , (1.845 sec/step)\n",
      "Train Epoch: 1 [139/214 (65%)] Loss:4.492130: , (1.852 sec/step)\n",
      "Train Epoch: 1 [140/214 (65%)] Loss:4.505379: , (1.849 sec/step)\n",
      "Train Epoch: 1 [141/214 (66%)] Loss:4.678392: , (1.855 sec/step)\n",
      "Train Epoch: 1 [142/214 (66%)] Loss:4.685012: , (1.856 sec/step)\n",
      "Train Epoch: 1 [143/214 (67%)] Loss:4.408503: , (1.851 sec/step)\n",
      "Train Epoch: 1 [144/214 (67%)] Loss:4.705737: , (1.861 sec/step)\n",
      "Train Epoch: 1 [145/214 (68%)] Loss:4.737027: , (1.863 sec/step)\n",
      "Train Epoch: 1 [146/214 (68%)] Loss:4.684360: , (1.852 sec/step)\n",
      "Train Epoch: 1 [147/214 (69%)] Loss:4.527120: , (1.853 sec/step)\n",
      "Train Epoch: 1 [148/214 (69%)] Loss:4.483315: , (1.852 sec/step)\n",
      "Train Epoch: 1 [149/214 (70%)] Loss:4.452630: , (1.852 sec/step)\n",
      "Train Epoch: 1 [150/214 (70%)] Loss:4.500135: , (1.863 sec/step)\n",
      "Train Epoch: 1 [151/214 (71%)] Loss:4.300500: , (1.852 sec/step)\n",
      "Train Epoch: 1 [152/214 (71%)] Loss:4.423622: , (1.854 sec/step)\n",
      "Train Epoch: 1 [153/214 (71%)] Loss:4.371000: , (1.853 sec/step)\n",
      "Train Epoch: 1 [154/214 (72%)] Loss:4.485996: , (1.859 sec/step)\n",
      "Train Epoch: 1 [155/214 (72%)] Loss:4.443991: , (1.860 sec/step)\n",
      "Train Epoch: 1 [156/214 (73%)] Loss:4.663872: , (1.864 sec/step)\n",
      "Train Epoch: 1 [157/214 (73%)] Loss:4.547359: , (1.866 sec/step)\n",
      "Train Epoch: 1 [158/214 (74%)] Loss:4.261245: , (1.864 sec/step)\n",
      "Train Epoch: 1 [159/214 (74%)] Loss:4.630550: , (1.852 sec/step)\n",
      "Train Epoch: 1 [160/214 (75%)] Loss:4.696617: , (1.857 sec/step)\n",
      "Train Epoch: 1 [161/214 (75%)] Loss:4.546498: , (1.852 sec/step)\n",
      "Train Epoch: 1 [162/214 (76%)] Loss:4.392983: , (1.861 sec/step)\n",
      "Train Epoch: 1 [163/214 (76%)] Loss:4.247389: , (1.852 sec/step)\n",
      "Train Epoch: 1 [164/214 (77%)] Loss:4.553268: , (1.856 sec/step)\n",
      "Train Epoch: 1 [165/214 (77%)] Loss:4.597250: , (1.854 sec/step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [166/214 (78%)] Loss:4.580834: , (1.852 sec/step)\n",
      "Train Epoch: 1 [167/214 (78%)] Loss:4.665667: , (1.853 sec/step)\n",
      "Train Epoch: 1 [168/214 (79%)] Loss:4.358455: , (1.856 sec/step)\n",
      "Train Epoch: 1 [169/214 (79%)] Loss:4.483765: , (1.852 sec/step)\n",
      "Train Epoch: 1 [170/214 (79%)] Loss:4.903052: , (1.858 sec/step)\n",
      "Train Epoch: 1 [171/214 (80%)] Loss:4.540727: , (1.850 sec/step)\n",
      "Train Epoch: 1 [172/214 (80%)] Loss:4.675546: , (1.842 sec/step)\n",
      "Train Epoch: 1 [173/214 (81%)] Loss:4.561025: , (1.848 sec/step)\n",
      "Train Epoch: 1 [174/214 (81%)] Loss:4.614818: , (1.840 sec/step)\n",
      "Train Epoch: 1 [175/214 (82%)] Loss:4.454752: , (1.839 sec/step)\n",
      "Train Epoch: 1 [176/214 (82%)] Loss:4.408684: , (1.840 sec/step)\n",
      "Train Epoch: 1 [177/214 (83%)] Loss:4.594599: , (1.839 sec/step)\n",
      "Train Epoch: 1 [178/214 (83%)] Loss:4.504374: , (1.839 sec/step)\n",
      "Train Epoch: 1 [179/214 (84%)] Loss:4.701473: , (1.844 sec/step)\n",
      "Train Epoch: 1 [180/214 (84%)] Loss:4.355359: , (1.843 sec/step)\n",
      "Train Epoch: 1 [181/214 (85%)] Loss:4.654351: , (1.843 sec/step)\n",
      "Train Epoch: 1 [182/214 (85%)] Loss:4.495401: , (1.848 sec/step)\n",
      "Train Epoch: 1 [183/214 (86%)] Loss:4.342653: , (1.861 sec/step)\n",
      "Train Epoch: 1 [184/214 (86%)] Loss:4.377136: , (1.850 sec/step)\n",
      "Train Epoch: 1 [185/214 (86%)] Loss:4.412619: , (1.851 sec/step)\n",
      "Train Epoch: 1 [186/214 (87%)] Loss:5.132287: , (1.854 sec/step)\n",
      "Train Epoch: 1 [187/214 (87%)] Loss:4.613731: , (1.851 sec/step)\n",
      "Train Epoch: 1 [188/214 (88%)] Loss:4.453313: , (1.854 sec/step)\n",
      "Train Epoch: 1 [189/214 (88%)] Loss:4.438633: , (1.851 sec/step)\n",
      "Train Epoch: 1 [190/214 (89%)] Loss:4.840431: , (1.851 sec/step)\n",
      "Train Epoch: 1 [191/214 (89%)] Loss:4.523358: , (1.851 sec/step)\n",
      "Train Epoch: 1 [192/214 (90%)] Loss:4.507694: , (1.849 sec/step)\n",
      "Train Epoch: 1 [193/214 (90%)] Loss:4.404799: , (1.851 sec/step)\n",
      "Train Epoch: 1 [194/214 (91%)] Loss:4.724504: , (1.860 sec/step)\n",
      "Train Epoch: 1 [195/214 (91%)] Loss:4.509686: , (1.859 sec/step)\n",
      "Train Epoch: 1 [196/214 (92%)] Loss:4.493331: , (1.854 sec/step)\n",
      "Train Epoch: 1 [197/214 (92%)] Loss:4.586458: , (1.851 sec/step)\n",
      "Train Epoch: 1 [198/214 (93%)] Loss:4.574168: , (1.858 sec/step)\n",
      "Train Epoch: 1 [199/214 (93%)] Loss:4.474342: , (1.844 sec/step)\n",
      "Train Epoch: 1 [200/214 (93%)] Loss:4.684750: , (1.847 sec/step)\n",
      "write finish\n",
      "Train Epoch: 1 [201/214 (94%)] Loss:4.439677: , (1.845 sec/step)\n",
      "Train Epoch: 1 [202/214 (94%)] Loss:4.550074: , (1.848 sec/step)\n",
      "Train Epoch: 1 [203/214 (95%)] Loss:4.559625: , (1.845 sec/step)\n",
      "Train Epoch: 1 [204/214 (95%)] Loss:4.320653: , (1.846 sec/step)\n",
      "Train Epoch: 1 [205/214 (96%)] Loss:4.458175: , (1.849 sec/step)\n",
      "Train Epoch: 1 [206/214 (96%)] Loss:4.761249: , (1.849 sec/step)\n",
      "Train Epoch: 1 [207/214 (97%)] Loss:4.614813: , (1.854 sec/step)\n",
      "Train Epoch: 1 [208/214 (97%)] Loss:4.492486: , (1.849 sec/step)\n",
      "Train Epoch: 1 [209/214 (98%)] Loss:4.567218: , (1.850 sec/step)\n",
      "Train Epoch: 1 [210/214 (98%)] Loss:4.665818: , (1.847 sec/step)\n",
      "Train Epoch: 1 [211/214 (99%)] Loss:4.521772: , (1.851 sec/step)\n",
      "Train Epoch: 1 [212/214 (99%)] Loss:4.525952: , (1.848 sec/step)\n",
      "Train Epoch: 1 [213/214 (100%)] Loss:4.448365: , (1.846 sec/step)\n",
      "0.0248125 accurate\n",
      "\n",
      "val set:loss4.4408:, (0.588 sec/step)\n",
      "\n",
      "val stored done 5.8873982429504395\n",
      "test stored done 74.41709685325623\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100000):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "gist": {
   "data": {
    "description": "MLalgorithm/mnistPyTorch.ipynb",
    "public": false
   },
   "id": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
