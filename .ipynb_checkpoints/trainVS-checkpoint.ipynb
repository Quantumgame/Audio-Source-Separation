{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T11:13:11.111761Z",
     "start_time": "2018-06-03T11:13:11.105551Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import scipy\n",
    "#rate,data = scipy.io.wavfile.read('./vocalSeparation/origin_mix.wav')\n",
    "#scipy.io.wavfile.write('./vocalSeparation/morigin_mix.wav',rate,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T11:13:11.118105Z",
     "start_time": "2018-06-03T11:13:11.114535Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import soundfile as sf\n",
    "#data, samplerate = sf.read('./vocalSeparation/pred_mix.wav', dtype='float32')\n",
    "#data = librosa.resample(data.T, samplerate, 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T11:13:13.034083Z",
     "start_time": "2018-06-03T11:13:11.119992Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import os\\nos.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Training script for the WaveNet network on the VCTK corpus.\n",
    "\n",
    "This script trains a network with the WaveNet using data from the VCTK corpus,\n",
    "which can be freely downloaded at the following site (~10 GB):\n",
    "http://homepages.inf.ed.ac.uk/jyamagis/page3/page58/page58.html\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import timeline\n",
    "\n",
    "from wavenetVS import WaveNetModel, AudioReader, optimizer_factory\n",
    "\n",
    "\n",
    "'''import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T11:13:13.047542Z",
     "start_time": "2018-06-03T11:13:13.036347Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "DATA_DIRECTORY = './vsCorpus'\n",
    "LOGDIR_ROOT = './logdirVS'\n",
    "CHECKPOINT_EVERY = 50\n",
    "NUM_STEPS = int(1e5)\n",
    "LEARNING_RATE = 1e-2\n",
    "WAVENET_PARAMS = './wavenet_params.json'\n",
    "STARTED_DATESTRING = \"{0:%Y-%m-%dT%H-%M-%S}\".format(datetime.now())\n",
    "SAMPLE_SIZE = 100000\n",
    "L2_REGULARIZATION_STRENGTH = 0\n",
    "#SILENCE_THRESHOLD = 0.3\n",
    "SILENCE_THRESHOLD = 0\n",
    "EPSILON = 0.001\n",
    "MOMENTUM = 0.9\n",
    "MAX_TO_KEEP = 5\n",
    "METADATA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T11:13:13.110886Z",
     "start_time": "2018-06-03T11:13:13.049553Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_arguments():\n",
    "    def _str_to_bool(s):\n",
    "        \"\"\"Convert string to bool (in argparse context).\"\"\"\n",
    "        if s.lower() not in ['true', 'false']:\n",
    "            raise ValueError('Argument needs to be a '\n",
    "                             'boolean, got {}'.format(s))\n",
    "        return {'true': True, 'false': False}[s.lower()]\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='WaveNet example network')\n",
    "    parser.add_argument('--batch_size', type=int, default=BATCH_SIZE,\n",
    "                        help='How many wav files to process at once. Default: ' + str(BATCH_SIZE) + '.')\n",
    "    parser.add_argument('--data_dir', type=str, default=DATA_DIRECTORY,\n",
    "                        help='The directory containing the VCTK corpus.')\n",
    "    parser.add_argument('--store_metadata', type=bool, default=METADATA,\n",
    "                        help='Whether to store advanced debugging information '\n",
    "                        '(execution time, memory consumption) for use with '\n",
    "                        'TensorBoard. Default: ' + str(METADATA) + '.')\n",
    "    parser.add_argument('--logdir', type=str, default=None,\n",
    "                        help='Directory in which to store the logging '\n",
    "                        'information for TensorBoard. '\n",
    "                        'If the model already exists, it will restore '\n",
    "                        'the state and will continue training. '\n",
    "                        'Cannot use with --logdir_root and --restore_from.')\n",
    "    parser.add_argument('--logdir_root', type=str, default=None,\n",
    "                        help='Root directory to place the logging '\n",
    "                        'output and generated model. These are stored '\n",
    "                        'under the dated subdirectory of --logdir_root. '\n",
    "                        'Cannot use with --logdir.')\n",
    "    parser.add_argument('--restore_from', type=str, default=None,\n",
    "                        help='Directory in which to restore the model from. '\n",
    "                        'This creates the new model under the dated directory '\n",
    "                        'in --logdir_root. '\n",
    "                        'Cannot use with --logdir.')\n",
    "    parser.add_argument('--checkpoint_every', type=int,\n",
    "                        default=CHECKPOINT_EVERY,\n",
    "                        help='How many steps to save each checkpoint after. Default: ' + str(CHECKPOINT_EVERY) + '.')\n",
    "    parser.add_argument('--num_steps', type=int, default=NUM_STEPS,\n",
    "                        help='Number of training steps. Default: ' + str(NUM_STEPS) + '.')\n",
    "    parser.add_argument('--learning_rate', type=float, default=LEARNING_RATE,\n",
    "                        help='Learning rate for training. Default: ' + str(LEARNING_RATE) + '.')\n",
    "    parser.add_argument('--wavenet_params', type=str, default=WAVENET_PARAMS,\n",
    "                        help='JSON file with the network parameters. Default: ' + WAVENET_PARAMS + '.')\n",
    "    parser.add_argument('--sample_size', type=int, default=SAMPLE_SIZE,\n",
    "                        help='Concatenate and cut audio samples to this many '\n",
    "                        'samples. Default: ' + str(SAMPLE_SIZE) + '.')\n",
    "    parser.add_argument('--l2_regularization_strength', type=float,\n",
    "                        default=L2_REGULARIZATION_STRENGTH,\n",
    "                        help='Coefficient in the L2 regularization. '\n",
    "                        'Default: False')\n",
    "    parser.add_argument('--silence_threshold', type=float,\n",
    "                        default=SILENCE_THRESHOLD,\n",
    "                        help='Volume threshold below which to trim the start '\n",
    "                        'and the end from the training set samples. Default: ' + str(SILENCE_THRESHOLD) + '.')\n",
    "    parser.add_argument('--optimizer', type=str, default='adam',\n",
    "                        choices=optimizer_factory.keys(),\n",
    "                        help='Select the optimizer specified by this option. Default: adam.')\n",
    "    parser.add_argument('--momentum', type=float,\n",
    "                        default=MOMENTUM, help='Specify the momentum to be '\n",
    "                        'used by sgd or rmsprop optimizer. Ignored by the '\n",
    "                        'adam optimizer. Default: ' + str(MOMENTUM) + '.')\n",
    "    parser.add_argument('--histograms', type=_str_to_bool, default=False,\n",
    "                        help='Whether to store histogram summaries. Default: False')\n",
    "    parser.add_argument('--gc_channels', type=int, default=None,\n",
    "                        help='Number of global condition channels. Default: None. Expecting: Int')\n",
    "    parser.add_argument('--max_checkpoints', type=int, default=MAX_TO_KEEP,\n",
    "                        help='Maximum amount of checkpoints that will be kept alive. Default: '\n",
    "                             + str(MAX_TO_KEEP) + '.')\n",
    "    return parser.parse_args([])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T11:13:13.121955Z",
     "start_time": "2018-06-03T11:13:13.112751Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save(saver, sess, logdir, step):\n",
    "    model_name = 'model.ckpt'\n",
    "    checkpoint_path = os.path.join(logdir, model_name)\n",
    "    print('Storing checkpoint to {} ...'.format(logdir), end=\"\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    if not os.path.exists(logdir):\n",
    "        os.makedirs(logdir)\n",
    "\n",
    "    saver.save(sess, checkpoint_path, global_step=step)\n",
    "    print(' Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T11:13:13.137755Z",
     "start_time": "2018-06-03T11:13:13.123872Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load(saver, sess, logdir):\n",
    "    print(\"Trying to restore saved checkpoints from {} ...\".format(logdir),\n",
    "          end=\"\")\n",
    "\n",
    "    ckpt = tf.train.get_checkpoint_state(logdir)\n",
    "    if ckpt:\n",
    "        print(\"  Checkpoint found: {}\".format(ckpt.model_checkpoint_path))\n",
    "        global_step = int(ckpt.model_checkpoint_path\n",
    "                          .split('/')[-1]\n",
    "                          .split('-')[-1])\n",
    "        print(\"  Global step was: {}\".format(global_step))\n",
    "        print(\"  Restoring...\", end=\"\")\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        print(\" Done.\")\n",
    "        return global_step\n",
    "    else:\n",
    "        print(\" No checkpoint found.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-06-03T11:13:11.112Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_default_logdir(logdir_root):\n",
    "    logdir = os.path.join(logdir_root, 'train', STARTED_DATESTRING)\n",
    "    return logdir\n",
    "\n",
    "\n",
    "def validate_directories(args):\n",
    "    \"\"\"Validate and arrange directory related arguments.\"\"\"\n",
    "\n",
    "    # Validation\n",
    "    if args.logdir and args.logdir_root:\n",
    "        raise ValueError(\"--logdir and --logdir_root cannot be \"\n",
    "                         \"specified at the same time.\")\n",
    "\n",
    "    if args.logdir and args.restore_from:\n",
    "        raise ValueError(\n",
    "            \"--logdir and --restore_from cannot be specified at the same \"\n",
    "            \"time. This is to keep your previous model from unexpected \"\n",
    "            \"overwrites.\\n\"\n",
    "            \"Use --logdir_root to specify the root of the directory which \"\n",
    "            \"will be automatically created with current date and time, or use \"\n",
    "            \"only --logdir to just continue the training from the last \"\n",
    "            \"checkpoint.\")\n",
    "\n",
    "    # Arrangement\n",
    "    logdir_root = args.logdir_root\n",
    "    if logdir_root is None:\n",
    "        logdir_root = LOGDIR_ROOT\n",
    "\n",
    "    logdir = args.logdir\n",
    "    if logdir is None:\n",
    "        logdir = get_default_logdir(logdir_root)\n",
    "        print('Using default logdir: {}'.format(logdir))\n",
    "\n",
    "    restore_from = args.restore_from\n",
    "    if restore_from is None:\n",
    "        # args.logdir and args.restore_from are exclusive,\n",
    "        # so it is guaranteed the logdir here is newly created.\n",
    "        restore_from = logdir\n",
    "\n",
    "    return {\n",
    "        'logdir': logdir,\n",
    "        'logdir_root': args.logdir_root,\n",
    "        'restore_from': restore_from\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-06-03T11:13:11.115Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default logdir: ./logdirVS/train/2018-06-03T19-13-13\n",
      "Trying to restore saved checkpoints from ./logdirVS/train/2018-06-03T19-13-13 ... No checkpoint found.\n",
      "step 0 - trloss = 5.710, (25.102 sec/step)\n",
      "validateLoss = 5.624, (25.102 sec/step)\n",
      "Storing checkpoint to ./logdirVS/train/2018-06-03T19-13-13 ... Done.\n",
      "step 1 - trloss = 5.673, (4.623 sec/step)\n",
      "step 2 - trloss = 5.637, (4.638 sec/step)\n",
      "step 3 - trloss = 5.606, (4.640 sec/step)\n",
      "step 4 - trloss = 5.578, (4.645 sec/step)\n",
      "step 5 - trloss = 5.553, (4.631 sec/step)\n",
      "step 6 - trloss = 5.531, (4.628 sec/step)\n",
      "step 7 - trloss = 5.510, (4.636 sec/step)\n",
      "step 8 - trloss = 5.491, (4.633 sec/step)\n",
      "step 9 - trloss = 5.474, (4.645 sec/step)\n",
      "step 10 - trloss = 5.457, (4.639 sec/step)\n",
      "step 11 - trloss = 5.441, (4.632 sec/step)\n",
      "step 12 - trloss = 5.426, (4.637 sec/step)\n",
      "step 13 - trloss = 5.413, (4.631 sec/step)\n",
      "step 14 - trloss = 5.401, (4.648 sec/step)\n",
      "step 15 - trloss = 5.391, (4.642 sec/step)\n",
      "step 16 - trloss = 5.382, (4.641 sec/step)\n",
      "step 17 - trloss = 5.372, (4.645 sec/step)\n",
      "step 18 - trloss = 5.363, (4.650 sec/step)\n",
      "step 19 - trloss = 5.361, (4.650 sec/step)\n",
      "step 20 - trloss = 5.353, (4.645 sec/step)\n",
      "step 21 - trloss = 5.348, (4.648 sec/step)\n",
      "step 22 - trloss = 5.341, (4.645 sec/step)\n",
      "step 23 - trloss = 5.336, (4.641 sec/step)\n",
      "step 24 - trloss = 5.332, (4.644 sec/step)\n",
      "step 25 - trloss = 5.328, (4.644 sec/step)\n",
      "step 26 - trloss = 5.322, (4.644 sec/step)\n",
      "step 27 - trloss = 5.318, (4.645 sec/step)\n",
      "step 28 - trloss = 5.313, (4.658 sec/step)\n",
      "step 29 - trloss = 5.310, (4.650 sec/step)\n",
      "step 30 - trloss = 5.305, (4.647 sec/step)\n",
      "step 31 - trloss = 5.303, (4.649 sec/step)\n",
      "step 32 - trloss = 5.298, (4.654 sec/step)\n",
      "step 33 - trloss = 5.295, (4.650 sec/step)\n",
      "step 34 - trloss = 5.291, (4.645 sec/step)\n",
      "step 35 - trloss = 5.288, (4.650 sec/step)\n",
      "step 36 - trloss = 5.284, (4.653 sec/step)\n",
      "step 37 - trloss = 5.281, (4.666 sec/step)\n",
      "step 38 - trloss = 5.276, (4.644 sec/step)\n",
      "step 39 - trloss = 5.273, (4.652 sec/step)\n",
      "step 40 - trloss = 5.269, (4.644 sec/step)\n",
      "step 41 - trloss = 5.265, (4.651 sec/step)\n",
      "step 42 - trloss = 5.261, (4.653 sec/step)\n",
      "step 43 - trloss = 5.257, (4.646 sec/step)\n",
      "step 44 - trloss = 5.253, (4.652 sec/step)\n",
      "step 45 - trloss = 5.248, (4.643 sec/step)\n",
      "step 46 - trloss = 5.244, (4.656 sec/step)\n",
      "step 47 - trloss = 5.240, (4.648 sec/step)\n",
      "step 48 - trloss = 5.235, (4.643 sec/step)\n",
      "step 49 - trloss = 5.231, (4.649 sec/step)\n",
      "step 50 - trloss = 5.227, (4.645 sec/step)\n",
      "validateLoss = 5.415, (4.645 sec/step)\n",
      "Storing checkpoint to ./logdirVS/train/2018-06-03T19-13-13 ... Done.\n",
      "step 51 - trloss = 5.225, (4.643 sec/step)\n",
      "step 52 - trloss = 5.223, (4.642 sec/step)\n",
      "step 53 - trloss = 5.213, (4.651 sec/step)\n",
      "step 54 - trloss = 5.205, (4.645 sec/step)\n",
      "step 55 - trloss = 5.202, (4.641 sec/step)\n",
      "step 56 - trloss = 5.196, (4.647 sec/step)\n",
      "step 57 - trloss = 5.186, (4.647 sec/step)\n",
      "step 58 - trloss = 5.179, (4.645 sec/step)\n",
      "step 59 - trloss = 5.171, (4.644 sec/step)\n",
      "step 60 - trloss = 5.166, (4.648 sec/step)\n",
      "step 61 - trloss = 5.158, (4.638 sec/step)\n",
      "step 62 - trloss = 5.148, (4.639 sec/step)\n",
      "step 63 - trloss = 5.133, (4.647 sec/step)\n",
      "step 64 - trloss = 5.120, (4.650 sec/step)\n",
      "step 65 - trloss = 5.106, (4.639 sec/step)\n",
      "step 66 - trloss = 5.093, (4.641 sec/step)\n",
      "step 67 - trloss = 5.081, (4.646 sec/step)\n",
      "step 68 - trloss = 5.082, (4.643 sec/step)\n",
      "step 69 - trloss = 5.180, (4.643 sec/step)\n",
      "step 70 - trloss = 5.154, (4.645 sec/step)\n",
      "step 71 - trloss = 5.075, (4.648 sec/step)\n",
      "step 72 - trloss = 5.092, (4.643 sec/step)\n",
      "step 73 - trloss = 5.071, (4.648 sec/step)\n",
      "step 74 - trloss = 5.038, (4.650 sec/step)\n",
      "step 75 - trloss = 5.060, (4.639 sec/step)\n",
      "step 76 - trloss = 5.006, (4.650 sec/step)\n",
      "step 77 - trloss = 5.023, (4.663 sec/step)\n",
      "step 78 - trloss = 5.009, (4.650 sec/step)\n",
      "step 79 - trloss = 4.980, (4.642 sec/step)\n",
      "step 80 - trloss = 4.993, (4.645 sec/step)\n",
      "step 81 - trloss = 4.961, (4.650 sec/step)\n",
      "step 82 - trloss = 4.965, (4.642 sec/step)\n",
      "step 83 - trloss = 4.946, (4.641 sec/step)\n",
      "step 84 - trloss = 4.938, (4.649 sec/step)\n",
      "step 85 - trloss = 4.932, (4.656 sec/step)\n",
      "step 86 - trloss = 4.908, (4.651 sec/step)\n",
      "step 87 - trloss = 4.911, (4.658 sec/step)\n",
      "step 88 - trloss = 4.892, (4.645 sec/step)\n",
      "step 89 - trloss = 4.888, (4.648 sec/step)\n",
      "step 90 - trloss = 4.870, (4.645 sec/step)\n",
      "step 91 - trloss = 4.867, (4.663 sec/step)\n",
      "step 92 - trloss = 4.853, (4.651 sec/step)\n",
      "step 93 - trloss = 4.842, (4.646 sec/step)\n",
      "step 94 - trloss = 4.837, (4.643 sec/step)\n",
      "step 95 - trloss = 4.819, (4.637 sec/step)\n",
      "step 96 - trloss = 4.814, (4.644 sec/step)\n",
      "step 97 - trloss = 4.803, (4.645 sec/step)\n",
      "step 98 - trloss = 4.788, (4.654 sec/step)\n",
      "step 99 - trloss = 4.782, (4.642 sec/step)\n",
      "step 100 - trloss = 4.772, (4.643 sec/step)\n",
      "validateLoss = 5.438, (4.643 sec/step)\n",
      "step 101 - trloss = 4.757, (4.651 sec/step)\n",
      "step 102 - trloss = 4.748, (4.643 sec/step)\n",
      "step 103 - trloss = 4.741, (4.657 sec/step)\n",
      "step 104 - trloss = 4.729, (4.646 sec/step)\n",
      "step 105 - trloss = 4.716, (4.643 sec/step)\n",
      "step 106 - trloss = 4.704, (4.642 sec/step)\n",
      "step 107 - trloss = 4.692, (4.645 sec/step)\n",
      "step 108 - trloss = 4.683, (4.644 sec/step)\n",
      "step 109 - trloss = 4.676, (4.654 sec/step)\n",
      "step 110 - trloss = 4.674, (4.652 sec/step)\n",
      "step 111 - trloss = 4.689, (4.647 sec/step)\n",
      "step 112 - trloss = 4.690, (4.652 sec/step)\n",
      "step 113 - trloss = 4.673, (4.642 sec/step)\n",
      "step 114 - trloss = 4.619, (4.650 sec/step)\n",
      "step 115 - trloss = 4.643, (4.642 sec/step)\n",
      "step 116 - trloss = 4.663, (4.653 sec/step)\n",
      "step 117 - trloss = 4.589, (4.660 sec/step)\n",
      "step 118 - trloss = 4.620, (4.642 sec/step)\n",
      "step 119 - trloss = 4.612, (4.653 sec/step)\n",
      "step 120 - trloss = 4.567, (4.642 sec/step)\n",
      "step 121 - trloss = 4.609, (4.661 sec/step)\n",
      "step 122 - trloss = 4.550, (4.651 sec/step)\n",
      "step 123 - trloss = 4.554, (4.646 sec/step)\n",
      "step 124 - trloss = 4.542, (4.665 sec/step)\n",
      "step 125 - trloss = 4.507, (4.660 sec/step)\n",
      "step 126 - trloss = 4.521, (4.649 sec/step)\n",
      "step 127 - trloss = 4.484, (4.656 sec/step)\n",
      "step 128 - trloss = 4.492, (4.649 sec/step)\n",
      "step 129 - trloss = 4.479, (4.657 sec/step)\n",
      "step 130 - trloss = 4.450, (4.644 sec/step)\n",
      "step 131 - trloss = 4.462, (4.645 sec/step)\n",
      "step 132 - trloss = 4.433, (4.645 sec/step)\n",
      "step 133 - trloss = 4.418, (4.661 sec/step)\n",
      "step 134 - trloss = 4.422, (4.656 sec/step)\n",
      "step 135 - trloss = 4.398, (4.644 sec/step)\n",
      "step 136 - trloss = 4.379, (4.652 sec/step)\n",
      "step 137 - trloss = 4.384, (4.652 sec/step)\n",
      "step 138 - trloss = 4.366, (4.652 sec/step)\n",
      "step 139 - trloss = 4.341, (4.649 sec/step)\n",
      "step 140 - trloss = 4.339, (4.646 sec/step)\n",
      "step 141 - trloss = 4.337, (4.660 sec/step)\n"
     ]
    }
   ],
   "source": [
    "args = get_arguments()\n",
    "\n",
    "try:\n",
    "    directories = validate_directories(args)\n",
    "except ValueError as e:\n",
    "    print(\"Some arguments are wrong:\")\n",
    "    print(str(e))\n",
    "\n",
    "logdir = directories['logdir']\n",
    "restore_from = directories['restore_from']\n",
    "\n",
    "# Even if we restored the model, we will treat it as new training\n",
    "# if the trained model is written into an arbitrary location.\n",
    "is_overwritten_training = logdir != restore_from\n",
    "\n",
    "with open(args.wavenet_params, 'r') as f:\n",
    "    wavenet_params = json.load(f)\n",
    "\n",
    "# Create coordinator.\n",
    "coord = tf.train.Coordinator()\n",
    "\n",
    "# Load raw waveform from VCTK corpus.\n",
    "with tf.name_scope('create_inputs'):\n",
    "    # Allow silence trimming to be skipped by specifying a threshold near\n",
    "    # zero.\n",
    "    silence_threshold = args.silence_threshold if args.silence_threshold > \\\n",
    "                                                  EPSILON else None\n",
    "    gc_enabled = args.gc_channels is not None\n",
    "    reader = AudioReader(\n",
    "        args.data_dir,\n",
    "        coord,\n",
    "        sample_rate=wavenet_params['sample_rate'],   #\"sample_rate\": 16000,\n",
    "        gc_enabled=gc_enabled,\n",
    "        receptive_field=WaveNetModel.calculate_receptive_field(wavenet_params[\"filter_width\"],\n",
    "                                                               wavenet_params[\"dilations\"],\n",
    "                                                               wavenet_params[\"scalar_input\"],\n",
    "                                                               wavenet_params[\"initial_filter_width\"]),\n",
    "        sample_size=args.sample_size,  #SAMPLE_SIZE = 100000\n",
    "        silence_threshold=silence_threshold)\n",
    "    audio_batch = reader.trdequeue(args.batch_size)  #BATCH_SIZE = 1\n",
    "    valaudio_batch = reader.vdequeue(args.batch_size)  #BATCH_SIZE = 1\n",
    "    if gc_enabled:\n",
    "        ##TODO train and val\n",
    "        gc_id_batch = reader.dequeue_gc(args.batch_size)\n",
    "    else:\n",
    "        gc_id_batch = None\n",
    "\n",
    "# Create network.\n",
    "net = WaveNetModel(\n",
    "    batch_size=args.batch_size,\n",
    "    dilations=wavenet_params[\"dilations\"],\n",
    "    filter_width=wavenet_params[\"filter_width\"],\n",
    "    residual_channels=wavenet_params[\"residual_channels\"],\n",
    "    dilation_channels=wavenet_params[\"dilation_channels\"],\n",
    "    skip_channels=wavenet_params[\"skip_channels\"],\n",
    "    quantization_channels=wavenet_params[\"quantization_channels\"],\n",
    "    use_biases=wavenet_params[\"use_biases\"],\n",
    "    scalar_input=wavenet_params[\"scalar_input\"],\n",
    "    initial_filter_width=wavenet_params[\"initial_filter_width\"],\n",
    "    histograms=args.histograms,\n",
    "    global_condition_channels=args.gc_channels,\n",
    "    global_condition_cardinality=reader.gc_category_cardinality)\n",
    "\n",
    "if args.l2_regularization_strength == 0:\n",
    "    args.l2_regularization_strength = None\n",
    "valloss = net.valloss(input_batch=valaudio_batch,\n",
    "                global_condition_batch=gc_id_batch,\n",
    "                l2_regularization_strength=args.l2_regularization_strength)\n",
    "trloss = net.trloss(input_batch=audio_batch,\n",
    "                global_condition_batch=gc_id_batch,\n",
    "                l2_regularization_strength=args.l2_regularization_strength)\n",
    "optimizer = optimizer_factory[args.optimizer](\n",
    "                learning_rate=args.learning_rate,\n",
    "                momentum=args.momentum)\n",
    "trainable = tf.trainable_variables()\n",
    "optim = optimizer.minimize(trloss, var_list=trainable)\n",
    "\n",
    "# Set up logging for TensorBoard.\n",
    "writer = tf.summary.FileWriter(logdir)\n",
    "writer.add_graph(tf.get_default_graph())\n",
    "run_metadata = tf.RunMetadata()\n",
    "summaries = tf.summary.merge_all()\n",
    "\n",
    "# Set up session\n",
    "#config = tf.ConfigProto(log_device_placement=False)\n",
    "#config.gpu_options.allow_growth=True\n",
    "#sess = tf.Session(config=config)\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=False))\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "# Saver for storing checkpoints of the model.\n",
    "saver = tf.train.Saver(var_list=tf.trainable_variables(), max_to_keep=args.max_checkpoints)\n",
    "\n",
    "try:\n",
    "    saved_global_step = load(saver, sess, restore_from)\n",
    "    if is_overwritten_training or saved_global_step is None:\n",
    "        # The first training step will be saved_global_step + 1,\n",
    "        # therefore we put -1 here for new or overwritten trainings.\n",
    "        saved_global_step = -1\n",
    "\n",
    "except:\n",
    "    print(\"Something went wrong while restoring checkpoint. \"\n",
    "          \"We will terminate training to avoid accidentally overwriting \"\n",
    "          \"the previous model.\")\n",
    "    raise\n",
    "\n",
    "    \n",
    "    \n",
    "threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "reader.start_threads(sess)\n",
    "\n",
    "step = None\n",
    "last_saved_step = saved_global_step\n",
    "minvalloss = 10000\n",
    "try:\n",
    "    for step in range(saved_global_step + 1, args.num_steps):\n",
    "        start_time = time.time()\n",
    "        if args.store_metadata and step % 50 == 0:\n",
    "            # Slow run that stores extra information for debugging.\n",
    "            print('Storing metadata')\n",
    "            run_options = tf.RunOptions(\n",
    "                trace_level=tf.RunOptions.FULL_TRACE)\n",
    "            summary, trloss_value, _ = sess.run(\n",
    "                [summaries, trloss, optim],\n",
    "                options=run_options,\n",
    "                run_metadata=run_metadata)\n",
    "            writer.add_summary(summary, step)\n",
    "            writer.add_run_metadata(run_metadata,\n",
    "                                    'step_{:04d}'.format(step))\n",
    "            tl = timeline.Timeline(run_metadata.step_stats)\n",
    "            timeline_path = os.path.join(logdir, 'timeline.trace')\n",
    "            with open(timeline_path, 'w') as f:\n",
    "                f.write(tl.generate_chrome_trace_format(show_memory=True))\n",
    "        else:\n",
    "            summary, trloss_value, _ = sess.run([summaries, trloss, optim])\n",
    "            writer.add_summary(summary, step)\n",
    "        duration = time.time() - start_time\n",
    "        print('step {:d} - trloss = {:.3f}, ({:.3f} sec/step)'\n",
    "              .format(step, trloss_value, duration))\n",
    "        \n",
    "        \n",
    "        if step % args.checkpoint_every == 0:\n",
    "            valloss_value = sess.run(valloss)\n",
    "            print('validateLoss = {:.3f}, ({:.3f} sec/step)'\n",
    "              .format(valloss_value, duration))\n",
    "            if(valloss_value < minvalloss):\n",
    "                save(saver, sess, logdir, step)\n",
    "                last_saved_step = step\n",
    "                minvalloss = valloss_value\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    # Introduce a line break after ^C is displayed so save message\n",
    "    # is on its own line.\n",
    "    print()\n",
    "finally:\n",
    "    if step > last_saved_step:\n",
    "        save(saver, sess, logdir, step)\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-06-03T11:13:11.116Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#step 3000---loss 4.250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
