{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T14:17:17.919854Z",
     "start_time": "2018-06-13T14:17:16.787049Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import torch.utils.data as utils\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import time\n",
    "import os\n",
    "from torch.utils import data\n",
    "from wavenet import Wavenet\n",
    "from transformData import x_mu_law_encode,y_mu_law_encode,mu_law_decode,onehot,cateToSignal\n",
    "from readDataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T14:17:17.933047Z",
     "start_time": "2018-06-13T14:17:17.922198Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sampleSize=32000#the length of the sample size\n",
    "quantization_channels=256\n",
    "sample_rate=16000\n",
    "dilations=[2**i for i in range(9)]*3  #idea from wavenet, have more receptive field\n",
    "residualDim=128 #\n",
    "skipDim=512\n",
    "shapeoftest = 190500\n",
    "filterSize=3\n",
    "resumefile='ccmix' # name of checkpoint\n",
    "lossname='ccmixloss.txt' # name of loss file\n",
    "continueTrain=False # whether use checkpoint\n",
    "pad = np.sum(dilations) # padding for dilate convolutional layers\n",
    "lossrecord=[]  #list for record loss\n",
    "#pad=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T14:17:17.937549Z",
     "start_time": "2018-06-13T14:17:17.934658Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"  # use specific GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T14:17:17.950659Z",
     "start_time": "2018-06-13T14:17:17.939056Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "torch.manual_seed(1)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "#torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T14:17:17.957668Z",
     "start_time": "2018-06-13T14:17:17.952197Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {'batch_size': 1,'shuffle': True,'num_workers': 2}\n",
    "\n",
    "training_set = Dataset(np.arange(0,3),np.arange(0,3),'ccmixter/x/','ccmixter/y/')\n",
    "validation_set=Dataset(np.arange(3,5),np.arange(3,5),'ccmixter/x/','ccmixter/y/')\n",
    "loadtr = data.DataLoader(training_set, **params)\n",
    "loadval = data.DataLoader(validation_set, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T14:17:22.636090Z",
     "start_time": "2018-06-13T14:17:17.959177Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Wavenet(pad,skipDim,quantization_channels,residualDim,dilations).cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#in wavenet paper, they said crossentropyloss is far better than MSELoss\n",
    "#optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3,weight_decay=1e-5)\n",
    "#use adam to train\n",
    "#optimizer = optim.SGD(model.parameters(), lr = 0.1, momentum=0.9, weight_decay=1e-5)\n",
    "#scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "#scheduler = MultiStepLR(optimizer, milestones=[20,40], gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T14:17:22.647658Z",
     "start_time": "2018-06-13T14:17:22.638686Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if continueTrain:# if continueTrain, the program will find the checkpoints\n",
    "    if os.path.isfile(resumefile):\n",
    "        print(\"=> loading checkpoint '{}'\".format(resumefile))\n",
    "        checkpoint = torch.load(resumefile)\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        #best_prec1 = checkpoint['best_prec1']\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "              .format(resumefile, checkpoint['epoch']))\n",
    "    else:\n",
    "        print(\"=> no checkpoint found at '{}'\".format(resumefile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-06-13T14:17:16.749Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def val(): # validation set\n",
    "    model.eval()\n",
    "    startval_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for iloader,(xtrain,ytrain) in enumerate(loadval):\n",
    "            idx = np.arange(pad,xtrain.shape[-1]-pad-sampleSize,1000)\n",
    "            np.random.shuffle(idx)\n",
    "            data = xtrain[:,:,idx[0]-pad:pad+idx[0]+sampleSize].to(device)\n",
    "            target = ytrain[:,idx[0]:idx[0]+sampleSize].to(device)\n",
    "            output = model(data)\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct = pred.eq(target.view_as(pred)).sum().item() / pred.shape[-1]\n",
    "            val_loss = criterion(output, target).item()\n",
    "            print(correct,'accurate')\n",
    "            print('\\nval set:loss{:.4f}:, ({:.3f} sec/step)\\n'.format(val_loss,time.time()-startval_time))\n",
    "\n",
    "            listofpred = []\n",
    "            for ind in range(pad,xtrain.shape[-1]-pad-sampleSize,sampleSize):\n",
    "                output = model(xtrain[:, :, ind - pad:ind + sampleSize + pad].to(device))\n",
    "                pred = output.max(1, keepdim=True)[1].cpu().numpy().reshape(-1)\n",
    "                listofpred.append(pred)\n",
    "            ans = mu_law_decode(np.concatenate(listofpred))\n",
    "            sf.write('./vsCorpus/noteval.wav', ans, sample_rate)\n",
    "            break\n",
    "    \n",
    "def train(epoch): #training set\n",
    "    model.train()\n",
    "    for iloader,(xtrain,ytrain) in enumerate(loadtr):\n",
    "        idx = np.arange(pad,xtrain.shape[-1]-pad-sampleSize,16000)\n",
    "        np.random.shuffle(idx)#random the starting points\n",
    "        for i, ind in enumerate(idx):\n",
    "            start_time = time.time()\n",
    "            data, target = xtrain[:,:,ind-pad:ind+sampleSize+pad].to(device), ytrain[:,ind:ind+sampleSize].to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            lossrecord.append(loss.item())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print('Train Epoch: {} iloader:{} [{}/{} ({:.0f}%)] Loss:{:.6f}: , ({:.3f} sec/step)'.format(\n",
    "                    epoch, iloader,i, len(idx),100. * i / len(idx), loss.item(),time.time() - start_time))\n",
    "            if i % 100 == 0:\n",
    "                with open(\"./lossRecord/\"+lossname, \"w\") as f:\n",
    "                    for s in lossrecord:\n",
    "                        f.write(str(s) +\"\\n\")\n",
    "                print('write finish')\n",
    "                state={'epoch': epoch + 1,\n",
    "                    'state_dict': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict()}\n",
    "                #torch.save(state, './model/'+resumefile)\n",
    "        val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-06-13T14:17:16.755Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 iloader:0 [0/167 (0%)] Loss:5.552440: , (2.146 sec/step)\n",
      "write finish\n",
      "Train Epoch: 0 iloader:0 [1/167 (1%)] Loss:5.479185: , (1.265 sec/step)\n",
      "Train Epoch: 0 iloader:0 [2/167 (1%)] Loss:5.774625: , (1.264 sec/step)\n",
      "Train Epoch: 0 iloader:0 [3/167 (2%)] Loss:5.461481: , (1.264 sec/step)\n",
      "Train Epoch: 0 iloader:0 [4/167 (2%)] Loss:5.368350: , (1.265 sec/step)\n",
      "Train Epoch: 0 iloader:0 [5/167 (3%)] Loss:5.396144: , (1.264 sec/step)\n",
      "Train Epoch: 0 iloader:0 [6/167 (4%)] Loss:5.752612: , (1.266 sec/step)\n",
      "Train Epoch: 0 iloader:0 [7/167 (4%)] Loss:5.005978: , (1.262 sec/step)\n",
      "Train Epoch: 0 iloader:0 [8/167 (5%)] Loss:5.367545: , (1.923 sec/step)\n",
      "Train Epoch: 0 iloader:0 [9/167 (5%)] Loss:5.534677: , (2.522 sec/step)\n",
      "Train Epoch: 0 iloader:0 [10/167 (6%)] Loss:5.449419: , (2.536 sec/step)\n",
      "Train Epoch: 0 iloader:0 [11/167 (7%)] Loss:3.386994: , (2.555 sec/step)\n",
      "Train Epoch: 0 iloader:0 [12/167 (7%)] Loss:2.227281: , (2.562 sec/step)\n",
      "Train Epoch: 0 iloader:0 [13/167 (8%)] Loss:0.884239: , (2.476 sec/step)\n",
      "Train Epoch: 0 iloader:0 [14/167 (8%)] Loss:0.704272: , (2.485 sec/step)\n",
      "Train Epoch: 0 iloader:0 [15/167 (9%)] Loss:17.180088: , (2.528 sec/step)\n",
      "Train Epoch: 0 iloader:0 [16/167 (10%)] Loss:12.125316: , (2.543 sec/step)\n",
      "Train Epoch: 0 iloader:0 [17/167 (10%)] Loss:8.320940: , (2.544 sec/step)\n",
      "Train Epoch: 0 iloader:0 [18/167 (11%)] Loss:6.078761: , (2.543 sec/step)\n",
      "Train Epoch: 0 iloader:0 [19/167 (11%)] Loss:2.626515: , (2.549 sec/step)\n",
      "Train Epoch: 0 iloader:0 [20/167 (12%)] Loss:3.003927: , (2.505 sec/step)\n",
      "Train Epoch: 0 iloader:0 [21/167 (13%)] Loss:5.499397: , (2.446 sec/step)\n",
      "Train Epoch: 0 iloader:0 [22/167 (13%)] Loss:5.437940: , (2.536 sec/step)\n",
      "Train Epoch: 0 iloader:0 [23/167 (14%)] Loss:5.439250: , (2.518 sec/step)\n",
      "Train Epoch: 0 iloader:0 [24/167 (14%)] Loss:5.297918: , (2.523 sec/step)\n",
      "Train Epoch: 0 iloader:0 [25/167 (15%)] Loss:5.289851: , (2.540 sec/step)\n",
      "Train Epoch: 0 iloader:0 [26/167 (16%)] Loss:5.219053: , (2.546 sec/step)\n",
      "Train Epoch: 0 iloader:0 [27/167 (16%)] Loss:5.215387: , (2.541 sec/step)\n",
      "Train Epoch: 0 iloader:0 [28/167 (17%)] Loss:5.280055: , (2.431 sec/step)\n",
      "Train Epoch: 0 iloader:0 [29/167 (17%)] Loss:4.304105: , (2.504 sec/step)\n",
      "Train Epoch: 0 iloader:0 [30/167 (18%)] Loss:4.190042: , (2.530 sec/step)\n",
      "Train Epoch: 0 iloader:0 [31/167 (19%)] Loss:5.336124: , (2.524 sec/step)\n",
      "Train Epoch: 0 iloader:0 [32/167 (19%)] Loss:5.376939: , (2.519 sec/step)\n",
      "Train Epoch: 0 iloader:0 [33/167 (20%)] Loss:5.251711: , (2.544 sec/step)\n",
      "Train Epoch: 0 iloader:0 [34/167 (20%)] Loss:3.278345: , (2.526 sec/step)\n",
      "Train Epoch: 0 iloader:0 [35/167 (21%)] Loss:5.378809: , (2.532 sec/step)\n",
      "Train Epoch: 0 iloader:0 [36/167 (22%)] Loss:5.403755: , (2.411 sec/step)\n",
      "Train Epoch: 0 iloader:0 [37/167 (22%)] Loss:5.472329: , (2.522 sec/step)\n",
      "Train Epoch: 0 iloader:0 [38/167 (23%)] Loss:5.352093: , (2.544 sec/step)\n",
      "Train Epoch: 0 iloader:0 [39/167 (23%)] Loss:2.868524: , (2.479 sec/step)\n",
      "Train Epoch: 0 iloader:0 [40/167 (24%)] Loss:5.295565: , (2.521 sec/step)\n",
      "Train Epoch: 0 iloader:0 [41/167 (25%)] Loss:5.349212: , (2.538 sec/step)\n",
      "Train Epoch: 0 iloader:0 [42/167 (25%)] Loss:5.314826: , (2.539 sec/step)\n",
      "Train Epoch: 0 iloader:0 [43/167 (26%)] Loss:5.301695: , (2.532 sec/step)\n",
      "Train Epoch: 0 iloader:0 [44/167 (26%)] Loss:2.718788: , (2.393 sec/step)\n",
      "Train Epoch: 0 iloader:0 [45/167 (27%)] Loss:5.184894: , (2.528 sec/step)\n",
      "Train Epoch: 0 iloader:0 [46/167 (28%)] Loss:5.340758: , (2.534 sec/step)\n",
      "Train Epoch: 0 iloader:0 [47/167 (28%)] Loss:5.315279: , (2.502 sec/step)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100000):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "gist": {
   "data": {
    "description": "MLalgorithm/mnistPyTorch.ipynb",
    "public": false
   },
   "id": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
