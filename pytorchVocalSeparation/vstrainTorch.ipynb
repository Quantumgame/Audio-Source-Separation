{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-06T14:48:29.862469Z",
     "start_time": "2018-06-06T14:48:29.848307Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import torch.utils.data as utils\n",
    "import librosa\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-06T14:48:30.119113Z",
     "start_time": "2018-06-06T14:48:30.091690Z"
    }
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch vocal seqera Example')\n",
    "parser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n",
    "                    help='SGD momentum (default: 0.5)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='disables CUDA training')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "args = parser.parse_args([])\n",
    "sampleSize=100000\n",
    "sample_rate=16000\n",
    "quantization_channels=256\n",
    "dilations=[2**i for i in range(9)]*3\n",
    "skipDim=512\n",
    "residualDim=32\n",
    "filterSize=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-06T14:48:30.614087Z",
     "start_time": "2018-06-06T14:48:30.603759Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "torch.manual_seed(args.seed)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "device = 'cpu'\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-06T14:48:31.182616Z",
     "start_time": "2018-06-06T14:48:31.154014Z"
    }
   },
   "outputs": [],
   "source": [
    "def mu_law_encode(audio, quantization_channels=quantization_channels):\n",
    "    '''Quantizes waveform amplitudes.'''\n",
    "    mu = (quantization_channels - 1)*1.0\n",
    "    # Perform mu-law companding transformation (ITU-T, 1988).\n",
    "    # Minimum operation is here to deal with rare large amplitudes caused\n",
    "    # by resampling.\n",
    "    safe_audio_abs = np.minimum(np.abs(audio), 1.0)\n",
    "    magnitude = np.log1p(mu * safe_audio_abs) / np.log1p(mu)\n",
    "    signal = np.sign(audio) * magnitude\n",
    "    # Quantize signal to the specified number of levels.\n",
    "    #if(forX):return signal\n",
    "    return ((signal + 1) / 2 * mu + 0.5).astype(int)\n",
    "def mu_law_decode(output, quantization_channels=quantization_channels):\n",
    "    '''Recovers waveform from quantized values.'''\n",
    "    mu = quantization_channels - 1\n",
    "    # Map values back to [-1, 1].\n",
    "    signal = 2 * ((output*1.0) / mu) - 1\n",
    "    # Perform inverse of mu-law transformation.\n",
    "    magnitude = (1 / mu) * ((1 + mu)**np.abs(signal) - 1)\n",
    "    return np.sign(signal) * magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-06T14:48:48.727796Z",
     "start_time": "2018-06-06T14:48:31.827420Z"
    }
   },
   "outputs": [],
   "source": [
    "def readAudio(name):\n",
    "    audio0, samplerate = sf.read(name, dtype='float32')\n",
    "    return librosa.resample(audio0.T, samplerate, sample_rate).reshape(-1)\n",
    "p=['../vsCorpus/origin_mix.wav','../vsCorpus/origin_mix.wav',\n",
    "   '../vsCorpus/origin_mix.wav','../vsCorpus/origin_mix.wav','../vsCorpus/pred_mix.wav']\n",
    "xtrain,ytrain,xval,yval,xtest=readAudio(p[0]),readAudio(p[1]),readAudio(p[2]),readAudio(p[3]),readAudio(p[4])\n",
    "xtrain,ytrain,xval,yval=xtrain[:-sampleSize],ytrain[:-sampleSize],xval[-sampleSize:],yval[-sampleSize:]\n",
    "xtrain,xval,xtest=xtrain.reshape(1,1,-1),xval.reshape(1,1,-1),xtest.reshape(1,1,-1)\n",
    "ytrain,yval=ytrain.reshape(1,-1),yval.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-06T14:48:48.775899Z",
     "start_time": "2018-06-06T14:48:48.730176Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xtrain=(xtrain-xtrain.mean())/xtrain.std()\n",
    "xval=(xval-xtrain.mean())/xtrain.std()\n",
    "xtest=(xtest-xtrain.mean())/xtrain.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-06T14:48:48.878119Z",
     "start_time": "2018-06-06T14:48:48.777877Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xtrain,ytrain,xval,yval,xtest=xtrain,mu_law_encode(ytrain),xval,mu_law_encode(yval),xtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-06T14:48:48.883918Z",
     "start_time": "2018-06-06T14:48:48.879979Z"
    }
   },
   "outputs": [],
   "source": [
    "xtrain,ytrain,xval,yval,xtest = torch.from_numpy(xtrain).type(torch.float32),\\\n",
    "                                torch.from_numpy(ytrain).type(torch.LongTensor),\\\n",
    "                                torch.from_numpy(xval).type(torch.float32),\\\n",
    "                                torch.from_numpy(yval).type(torch.LongTensor),\\\n",
    "                                torch.from_numpy(xtest).type(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-06T14:48:53.702113Z",
     "start_time": "2018-06-06T14:48:53.538957Z"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        sd,qd,rd = skipDim,quantization_channels,residualDim\n",
    "        self.causal = nn.Conv1d(in_channels=1,out_channels=rd,kernel_size=3,padding=1)\n",
    "        self.dilated=dict()\n",
    "        for i, d in enumerate(dilations):\n",
    "            self.dilated['tanh'+str(i)] = nn.Conv1d(in_channels=rd,out_channels=rd,kernel_size=3,padding=d,dilation=d)\n",
    "            self.dilated['sigmoid'+str(i)] = nn.Conv1d(in_channels=rd,out_channels=rd,kernel_size=3,padding=d,dilation=d)\n",
    "            self.dilated['skip'+str(i)] = nn.Conv1d(in_channels=rd,out_channels=sd,kernel_size=1,padding=0)\n",
    "            self.dilated['dense'+str(i)] = nn.Conv1d(in_channels=rd,out_channels=rd,kernel_size=1,padding=0)\n",
    "        self.post1 = nn.Conv1d(in_channels=sd,out_channels=sd,kernel_size=1,padding=0)\n",
    "        self.post2 = nn.Conv1d(in_channels=sd,out_channels=qd,kernel_size=1,padding=0)\n",
    "        self.tanh,self.sigmoid = nn.Tanh(),nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.causal(x)\n",
    "        skip_connections = torch.zeros([1,skipDim,x.shape[2]], dtype=torch.float32)\n",
    "        for i, dilation in enumerate(dilations):\n",
    "            xinput=x.clone()      \n",
    "            x1 = self.tanh(self.dilated['tanh'+str(i)](x))\n",
    "            x2 = self.sigmoid(self.dilated['sigmoid'+str(i)](x))\n",
    "            x = x1*x2\n",
    "            skip_connections += self.dilated['skip'+str(i)](x).clone()\n",
    "            x = self.dilated['dense'+str(i)](x)\n",
    "            x += xinput.clone()\n",
    "        x = F.relu(x)\n",
    "        x = self.post2(F.relu(self.post1(skip_connections)))\n",
    "        return F.log_softmax(x,dim=1)\n",
    "\n",
    "model = Net().to(device)\n",
    "\n",
    "#optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-06T14:54:25.718002Z",
     "start_time": "2018-06-06T14:54:25.646912Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def val():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        data, target = xval.to(device), yval.to(device)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).item() # sum up batch loss\n",
    "        pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= yval.shape[1]\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, yval.shape[1],100. * correct / yval.shape[1]))\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    idx = np.arange(xtrain.shape[-1] - sampleSize)\n",
    "    np.random.shuffle(idx)\n",
    "    for i, ind in enumerate(idx):\n",
    "        x = xtrain[:,:,ind:ind+sampleSize]\n",
    "        y = ytrain[:,ind:ind+sampleSize]\n",
    "        data, target = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, i, len(idx),100. * i / len(idx), loss.item()))\n",
    "        if i % 100 == 0:val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-06-06T14:54:29.363Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 4.766451835632324\n",
      "Train Epoch: 0 [0/3571161 (0%)]\tLoss: 4.766452\n",
      "loss 4.803569316864014\n",
      "loss 4.705507278442383\n",
      "loss 4.63505744934082\n",
      "loss 4.574368476867676\n",
      "loss 4.40049934387207\n",
      "loss 4.430263519287109\n",
      "loss 4.359923839569092\n",
      "loss 4.247335433959961\n",
      "loss 4.311568737030029\n",
      "loss 4.241927623748779\n",
      "loss 4.246124744415283\n",
      "loss 4.147509574890137\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    train(epoch)\n",
    "    val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "gist": {
   "data": {
    "description": "MLalgorithm/mnistPyTorch.ipynb",
    "public": false
   },
   "id": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
